{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"HIPO: Multi-Cloud LLM Infrastructure","text":"<p>HIPO (High-Performance Infrastructure for Optimized machine learning) is a  platform designed to provide a unified interface for deploying and managing machine learning models across multiple cloud providers.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Multi-cloud support (AWS, GCP, Azure)</li> <li>Secure model weights management</li> <li>Kubernetes orchestration</li> <li>Autoscaling for GPU workloads</li> <li>Centralized API gateway</li> </ul>"},{"location":"security-guide/","title":"Security Guide","text":"<p>This guide covers the security features and best practices for using HIPO, with a focus on securing ML model weights and sensitive configurations.</p>"},{"location":"security-guide/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Secure Model Weights Storage</li> <li>Encryption Features</li> <li>Multi-Cloud Storage</li> <li>Access Control</li> <li>Integrity Verification</li> <li>Secret Management</li> <li>Configuration</li> <li>Best Practices</li> </ul>"},{"location":"security-guide/#overview","title":"Overview","text":"<p>HIPO implements a comprehensive security model to protect your ML/LLM assets, with a focus on:</p> <ul> <li>End-to-end encryption of model weights</li> <li>Multi-cloud redundancy and secure storage</li> <li>Access control and audit logging</li> <li>Secret management across environments</li> <li>Secure key rotation</li> </ul>"},{"location":"security-guide/#secure-model-weights-storage","title":"Secure Model Weights Storage","text":"<p>The <code>SecureModelWeights</code> module provides a secure framework for storing, managing, and accessing ML model weights across multiple cloud providers.</p>"},{"location":"security-guide/#encryption-features","title":"Encryption Features","text":"<p>Model weights are encrypted before storage using strong cryptographic algorithms:</p> <pre><code>from src.security.encryption import EncryptionService\nfrom src.models.secure_weights import SecureModelWeights, create_secure_weights_manager\n\n# Create encryption service\nencryption_service = EncryptionService()\n\n# Create secure weights manager\nsecure_weights = create_secure_weights_manager(\n    config={'secure_weights': {'storage': {...}, 'cache': {...}}},\n    encryption_service=encryption_service\n)\n\n# Store weights with encryption\nmetadata = secure_weights.store_weights(\n    model_name='llama2-7b',\n    weights_file='/path/to/weights.bin',\n    encrypt=True  # Enable encryption\n)\n</code></pre> <p>Encryption uses AES-256 for symmetric encryption and RSA-2048 for key protection. Keys can be stored securely in the cloud provider's key management service or in a local secure directory.</p>"},{"location":"security-guide/#multi-cloud-storage","title":"Multi-Cloud Storage","text":"<p>Model weights can be stored redundantly across multiple cloud providers:</p> <pre><code># Configuration for multi-cloud storage\nconfig = {\n    'storage': {\n        'primary': 's3',  # Primary storage provider\n        'replicate_to': ['gcs', 'azure'],  # Secondary providers for replication\n        's3_bucket': 'llm-models',\n        'gcs_bucket': 'llm-models',\n        'azure_container': 'llm-models',\n        'versioning_enabled': True\n    }\n}\n\n# Create secure weights manager with multi-cloud config\nsecure_weights = create_secure_weights_manager(config={'secure_weights': config})\n\n# Store weights (will be replicated automatically)\nmetadata = secure_weights.store_weights(\n    model_name='llama2-7b',\n    weights_file='/path/to/weights.bin',\n    encrypt=True\n)\n\n# Verify storage locations\nfor location in metadata['storage_locations']:\n    print(f\"Stored in {location['provider']}\")\n</code></pre>"},{"location":"security-guide/#access-control","title":"Access Control","text":"<p>Access control can be enabled to restrict who can access and modify model weights:</p> <pre><code># Enable access control in configuration\nconfig = {\n    'storage': {\n        'primary': 's3',\n        'access_control_enabled': True,\n    }\n}\n\n# Create secure weights manager\nsecure_weights = create_secure_weights_manager(config={'secure_weights': config})\n</code></pre> <p>When access control is enabled, the system will: - Track all operations in an audit log - Require authentication for all operations - Enforce role-based permissions - Validate signatures for integrity</p>"},{"location":"security-guide/#integrity-verification","title":"Integrity Verification","text":"<p>The system automatically calculates checksums for model weights and verifies them on access:</p> <pre><code># Verify integrity of model weights\nresults = secure_weights.verify_weights_integrity(\n    model_name='llama2-7b',\n    version='v1.0'\n)\n\nfor location, verified in results.items():\n    if verified:\n        print(f\"\u2705 Integrity verified for {location}\")\n    else:\n        print(f\"\u274c Integrity check failed for {location}\")\n</code></pre>"},{"location":"security-guide/#secret-management","title":"Secret Management","text":"<p>The <code>SecretManager</code> provides a unified interface for managing secrets across cloud providers:</p> <pre><code>from src.secrets.secret_manager import SecretManager\n\n# Create secret manager\nsecret_manager = SecretManager(config, cloud_providers)\n\n# Store a secret across all providers\nsecret_manager.create_secret(\n    secret_name=\"api-keys\",\n    secret_data={\"openai\": \"sk-...\", \"anthropic\": \"sk-...\"}\n)\n\n# Get a secret\napi_keys = secret_manager.get_secret(\"api-keys\")\n\n# Automatic secret rotation\nsecret_manager.start()  # Start rotation thread\n</code></pre>"},{"location":"security-guide/#configuration","title":"Configuration","text":"<p>Configure security settings in your <code>config.yaml</code>:</p> <pre><code>security:\n  encryption:\n    key_directory: 'secrets'  # Directory to store encryption keys\n    load_keys_from_env: false # Load keys from environment variables\n\n  secure_weights:\n    enabled: true\n    storage:\n      primary: 's3'\n      replicate_to: ['gcs']\n      s3_bucket: 'llm-models'\n      versioning_enabled: true\n      checksum_algorithm: 'sha256'\n      access_control_enabled: true\n      encryption_enabled: true\n\nsecrets:\n  vault:\n    enabled: false\n    address: 'http://vault:8200'\n    auth_method: 'kubernetes'\n\n  model_weights:\n    storage_type: 's3'\n    s3_bucket: 'llm-models'\n    sync_enabled: true\n    versioning_enabled: true\n\n  rotation:\n    enabled: true\n    schedule: '0 0 * * 0'  # Weekly on Sunday\n</code></pre>"},{"location":"security-guide/#best-practices","title":"Best Practices","text":"<ol> <li>Always Enable Encryption: Use <code>encrypt=True</code> when storing model weights.</li> <li>Use Multi-Cloud Replication: Configure replication to at least one additional provider.</li> <li>Regular Key Rotation: Enable automatic key rotation in the configuration.</li> <li>Integrity Verification: Periodically run <code>verify_weights_integrity()</code> to check model integrity.</li> <li>Secure Local Cache: Configure cache cleanup to prevent accumulation of unneeded weights.</li> <li>Access Control: Enable access control and audit logging for all operations.</li> <li>Backup Keys: Ensure encryption keys are backed up securely.</li> <li>Environment Isolation: Use different buckets and configurations for dev/staging/prod.</li> </ol>"},{"location":"api/cloud/","title":"Cloud API","text":""},{"location":"api/cloud/#cloudprovider","title":"CloudProvider","text":""},{"location":"api/cloud/#src.cloud.provider.CloudProvider","title":"<code>src.cloud.provider.CloudProvider</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for cloud providers.</p> Source code in <code>src/cloud/provider.py</code> <pre><code>class CloudProvider(ABC):\n    \"\"\"Abstract base class for cloud providers.\"\"\"\n\n    def __init__(self, config: Dict[str, Any]):\n        \"\"\"Initialize cloud provider.\n\n        Args:\n            config: Cloud provider configuration.\n        \"\"\"\n        self.config = config\n        self.name = self.__class__.__name__\n        self.region = config.get(\"region\")\n        self.enabled = config.get(\"enabled\", True)\n        self.logger = logging.getLogger(f\"{__name__}.{self.name}\")\n\n    @abstractmethod\n    def get_kubernetes_client(self):\n        \"\"\"Get Kubernetes client for this cloud provider.\n\n        Returns:\n            Kubernetes client.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_kubernetes_config(self) -&gt; Dict[str, Any]:\n        \"\"\"Get Kubernetes configuration for this cloud provider.\n\n        Returns:\n            Kubernetes configuration.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def create_kubernetes_cluster(self) -&gt; str:\n        \"\"\"Create a Kubernetes cluster in this cloud provider.\n\n        Returns:\n            Cluster ID.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def delete_kubernetes_cluster(self, cluster_id: str) -&gt; bool:\n        \"\"\"Delete a Kubernetes cluster in this cloud provider.\n\n        Args:\n            cluster_id: Cluster ID.\n\n        Returns:\n            True if successful, False otherwise.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_node_groups(self) -&gt; List[Dict[str, Any]]:\n        \"\"\"Get node groups for this cloud provider.\n\n        Returns:\n            List of node groups.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def scale_node_group(self, node_group_id: str, desired_size: int) -&gt; bool:\n        \"\"\"Scale a node group to the desired size.\n\n        Args:\n            node_group_id: Node group ID.\n            desired_size: Desired size of the node group.\n\n        Returns:\n            True if successful, False otherwise.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_gpu_metrics(self, node_ids: Optional[List[str]] = None) -&gt; Dict[str, Any]:\n        \"\"\"Get GPU metrics for the specified nodes.\n\n        Args:\n            node_ids: List of node IDs to get metrics for. If None, get metrics for all nodes.\n\n        Returns:\n            Dictionary of GPU metrics.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_api_gateway(self):\n        \"\"\"Get API gateway for this cloud provider.\n\n        Returns:\n            API gateway client.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def create_api_gateway(self, name: str, description: str) -&gt; str:\n        \"\"\"Create an API gateway in this cloud provider.\n\n        Args:\n            name: API gateway name.\n            description: API gateway description.\n\n        Returns:\n            API gateway ID.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_secret_manager(self):\n        \"\"\"Get secret manager for this cloud provider.\n\n        Returns:\n            Secret manager client.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_secret(self, secret_name: str) -&gt; Dict[str, Any]:\n        \"\"\"Get a secret from this cloud provider.\n\n        Args:\n            secret_name: Secret name.\n\n        Returns:\n            Secret data.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def create_secret(self, secret_name: str, secret_data: Dict[str, Any]) -&gt; str:\n        \"\"\"Create a secret in this cloud provider.\n\n        Args:\n            secret_name: Secret name.\n            secret_data: Secret data.\n\n        Returns:\n            Secret ID.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def update_secret(self, secret_name: str, secret_data: Dict[str, Any]) -&gt; bool:\n        \"\"\"Update a secret in this cloud provider.\n\n        Args:\n            secret_name: Secret name.\n            secret_data: Secret data.\n\n        Returns:\n            True if successful, False otherwise.\n        \"\"\"\n        pass\n\n    @abstractmethod\n    def get_cost_metrics(self, timeframe: str = \"daily\") -&gt; Dict[str, float]:\n        \"\"\"Get cost metrics for this cloud provider.\n\n        Args:\n            timeframe: Timeframe for cost metrics. Options: hourly, daily, weekly, monthly.\n\n        Returns:\n            Dictionary of cost metrics.\n        \"\"\"\n        pass\n\n    def is_enabled(self) -&gt; bool:\n        \"\"\"Check if this cloud provider is enabled.\n\n        Returns:\n            True if enabled, False otherwise.\n        \"\"\"\n        return self.enabled\n\n    def __str__(self) -&gt; str:\n        \"\"\"String representation of this cloud provider.\n\n        Returns:\n            String representation.\n        \"\"\"\n        return f\"{self.name} ({self.region})\"\n</code></pre>"},{"location":"api/cloud/#src.cloud.provider.CloudProvider.__init__","title":"<code>__init__(config)</code>","text":"<p>Initialize cloud provider.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Dict[str, Any]</code> <p>Cloud provider configuration.</p> required Source code in <code>src/cloud/provider.py</code> <pre><code>def __init__(self, config: Dict[str, Any]):\n    \"\"\"Initialize cloud provider.\n\n    Args:\n        config: Cloud provider configuration.\n    \"\"\"\n    self.config = config\n    self.name = self.__class__.__name__\n    self.region = config.get(\"region\")\n    self.enabled = config.get(\"enabled\", True)\n    self.logger = logging.getLogger(f\"{__name__}.{self.name}\")\n</code></pre>"},{"location":"api/cloud/#src.cloud.provider.CloudProvider.__str__","title":"<code>__str__()</code>","text":"<p>String representation of this cloud provider.</p> <p>Returns:</p> Type Description <code>str</code> <p>String representation.</p> Source code in <code>src/cloud/provider.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"String representation of this cloud provider.\n\n    Returns:\n        String representation.\n    \"\"\"\n    return f\"{self.name} ({self.region})\"\n</code></pre>"},{"location":"api/cloud/#src.cloud.provider.CloudProvider.create_api_gateway","title":"<code>create_api_gateway(name, description)</code>  <code>abstractmethod</code>","text":"<p>Create an API gateway in this cloud provider.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>API gateway name.</p> required <code>description</code> <code>str</code> <p>API gateway description.</p> required <p>Returns:</p> Type Description <code>str</code> <p>API gateway ID.</p> Source code in <code>src/cloud/provider.py</code> <pre><code>@abstractmethod\ndef create_api_gateway(self, name: str, description: str) -&gt; str:\n    \"\"\"Create an API gateway in this cloud provider.\n\n    Args:\n        name: API gateway name.\n        description: API gateway description.\n\n    Returns:\n        API gateway ID.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/cloud/#src.cloud.provider.CloudProvider.create_kubernetes_cluster","title":"<code>create_kubernetes_cluster()</code>  <code>abstractmethod</code>","text":"<p>Create a Kubernetes cluster in this cloud provider.</p> <p>Returns:</p> Type Description <code>str</code> <p>Cluster ID.</p> Source code in <code>src/cloud/provider.py</code> <pre><code>@abstractmethod\ndef create_kubernetes_cluster(self) -&gt; str:\n    \"\"\"Create a Kubernetes cluster in this cloud provider.\n\n    Returns:\n        Cluster ID.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/cloud/#src.cloud.provider.CloudProvider.create_secret","title":"<code>create_secret(secret_name, secret_data)</code>  <code>abstractmethod</code>","text":"<p>Create a secret in this cloud provider.</p> <p>Parameters:</p> Name Type Description Default <code>secret_name</code> <code>str</code> <p>Secret name.</p> required <code>secret_data</code> <code>Dict[str, Any]</code> <p>Secret data.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Secret ID.</p> Source code in <code>src/cloud/provider.py</code> <pre><code>@abstractmethod\ndef create_secret(self, secret_name: str, secret_data: Dict[str, Any]) -&gt; str:\n    \"\"\"Create a secret in this cloud provider.\n\n    Args:\n        secret_name: Secret name.\n        secret_data: Secret data.\n\n    Returns:\n        Secret ID.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/cloud/#src.cloud.provider.CloudProvider.delete_kubernetes_cluster","title":"<code>delete_kubernetes_cluster(cluster_id)</code>  <code>abstractmethod</code>","text":"<p>Delete a Kubernetes cluster in this cloud provider.</p> <p>Parameters:</p> Name Type Description Default <code>cluster_id</code> <code>str</code> <p>Cluster ID.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if successful, False otherwise.</p> Source code in <code>src/cloud/provider.py</code> <pre><code>@abstractmethod\ndef delete_kubernetes_cluster(self, cluster_id: str) -&gt; bool:\n    \"\"\"Delete a Kubernetes cluster in this cloud provider.\n\n    Args:\n        cluster_id: Cluster ID.\n\n    Returns:\n        True if successful, False otherwise.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/cloud/#src.cloud.provider.CloudProvider.get_api_gateway","title":"<code>get_api_gateway()</code>  <code>abstractmethod</code>","text":"<p>Get API gateway for this cloud provider.</p> <p>Returns:</p> Type Description <p>API gateway client.</p> Source code in <code>src/cloud/provider.py</code> <pre><code>@abstractmethod\ndef get_api_gateway(self):\n    \"\"\"Get API gateway for this cloud provider.\n\n    Returns:\n        API gateway client.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/cloud/#src.cloud.provider.CloudProvider.get_cost_metrics","title":"<code>get_cost_metrics(timeframe='daily')</code>  <code>abstractmethod</code>","text":"<p>Get cost metrics for this cloud provider.</p> <p>Parameters:</p> Name Type Description Default <code>timeframe</code> <code>str</code> <p>Timeframe for cost metrics. Options: hourly, daily, weekly, monthly.</p> <code>'daily'</code> <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dictionary of cost metrics.</p> Source code in <code>src/cloud/provider.py</code> <pre><code>@abstractmethod\ndef get_cost_metrics(self, timeframe: str = \"daily\") -&gt; Dict[str, float]:\n    \"\"\"Get cost metrics for this cloud provider.\n\n    Args:\n        timeframe: Timeframe for cost metrics. Options: hourly, daily, weekly, monthly.\n\n    Returns:\n        Dictionary of cost metrics.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/cloud/#src.cloud.provider.CloudProvider.get_gpu_metrics","title":"<code>get_gpu_metrics(node_ids=None)</code>  <code>abstractmethod</code>","text":"<p>Get GPU metrics for the specified nodes.</p> <p>Parameters:</p> Name Type Description Default <code>node_ids</code> <code>Optional[List[str]]</code> <p>List of node IDs to get metrics for. If None, get metrics for all nodes.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary of GPU metrics.</p> Source code in <code>src/cloud/provider.py</code> <pre><code>@abstractmethod\ndef get_gpu_metrics(self, node_ids: Optional[List[str]] = None) -&gt; Dict[str, Any]:\n    \"\"\"Get GPU metrics for the specified nodes.\n\n    Args:\n        node_ids: List of node IDs to get metrics for. If None, get metrics for all nodes.\n\n    Returns:\n        Dictionary of GPU metrics.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/cloud/#src.cloud.provider.CloudProvider.get_kubernetes_client","title":"<code>get_kubernetes_client()</code>  <code>abstractmethod</code>","text":"<p>Get Kubernetes client for this cloud provider.</p> <p>Returns:</p> Type Description <p>Kubernetes client.</p> Source code in <code>src/cloud/provider.py</code> <pre><code>@abstractmethod\ndef get_kubernetes_client(self):\n    \"\"\"Get Kubernetes client for this cloud provider.\n\n    Returns:\n        Kubernetes client.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/cloud/#src.cloud.provider.CloudProvider.get_kubernetes_config","title":"<code>get_kubernetes_config()</code>  <code>abstractmethod</code>","text":"<p>Get Kubernetes configuration for this cloud provider.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Kubernetes configuration.</p> Source code in <code>src/cloud/provider.py</code> <pre><code>@abstractmethod\ndef get_kubernetes_config(self) -&gt; Dict[str, Any]:\n    \"\"\"Get Kubernetes configuration for this cloud provider.\n\n    Returns:\n        Kubernetes configuration.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/cloud/#src.cloud.provider.CloudProvider.get_node_groups","title":"<code>get_node_groups()</code>  <code>abstractmethod</code>","text":"<p>Get node groups for this cloud provider.</p> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of node groups.</p> Source code in <code>src/cloud/provider.py</code> <pre><code>@abstractmethod\ndef get_node_groups(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"Get node groups for this cloud provider.\n\n    Returns:\n        List of node groups.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/cloud/#src.cloud.provider.CloudProvider.get_secret","title":"<code>get_secret(secret_name)</code>  <code>abstractmethod</code>","text":"<p>Get a secret from this cloud provider.</p> <p>Parameters:</p> Name Type Description Default <code>secret_name</code> <code>str</code> <p>Secret name.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Secret data.</p> Source code in <code>src/cloud/provider.py</code> <pre><code>@abstractmethod\ndef get_secret(self, secret_name: str) -&gt; Dict[str, Any]:\n    \"\"\"Get a secret from this cloud provider.\n\n    Args:\n        secret_name: Secret name.\n\n    Returns:\n        Secret data.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/cloud/#src.cloud.provider.CloudProvider.get_secret_manager","title":"<code>get_secret_manager()</code>  <code>abstractmethod</code>","text":"<p>Get secret manager for this cloud provider.</p> <p>Returns:</p> Type Description <p>Secret manager client.</p> Source code in <code>src/cloud/provider.py</code> <pre><code>@abstractmethod\ndef get_secret_manager(self):\n    \"\"\"Get secret manager for this cloud provider.\n\n    Returns:\n        Secret manager client.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/cloud/#src.cloud.provider.CloudProvider.is_enabled","title":"<code>is_enabled()</code>","text":"<p>Check if this cloud provider is enabled.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if enabled, False otherwise.</p> Source code in <code>src/cloud/provider.py</code> <pre><code>def is_enabled(self) -&gt; bool:\n    \"\"\"Check if this cloud provider is enabled.\n\n    Returns:\n        True if enabled, False otherwise.\n    \"\"\"\n    return self.enabled\n</code></pre>"},{"location":"api/cloud/#src.cloud.provider.CloudProvider.scale_node_group","title":"<code>scale_node_group(node_group_id, desired_size)</code>  <code>abstractmethod</code>","text":"<p>Scale a node group to the desired size.</p> <p>Parameters:</p> Name Type Description Default <code>node_group_id</code> <code>str</code> <p>Node group ID.</p> required <code>desired_size</code> <code>int</code> <p>Desired size of the node group.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if successful, False otherwise.</p> Source code in <code>src/cloud/provider.py</code> <pre><code>@abstractmethod\ndef scale_node_group(self, node_group_id: str, desired_size: int) -&gt; bool:\n    \"\"\"Scale a node group to the desired size.\n\n    Args:\n        node_group_id: Node group ID.\n        desired_size: Desired size of the node group.\n\n    Returns:\n        True if successful, False otherwise.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/cloud/#src.cloud.provider.CloudProvider.update_secret","title":"<code>update_secret(secret_name, secret_data)</code>  <code>abstractmethod</code>","text":"<p>Update a secret in this cloud provider.</p> <p>Parameters:</p> Name Type Description Default <code>secret_name</code> <code>str</code> <p>Secret name.</p> required <code>secret_data</code> <code>Dict[str, Any]</code> <p>Secret data.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if successful, False otherwise.</p> Source code in <code>src/cloud/provider.py</code> <pre><code>@abstractmethod\ndef update_secret(self, secret_name: str, secret_data: Dict[str, Any]) -&gt; bool:\n    \"\"\"Update a secret in this cloud provider.\n\n    Args:\n        secret_name: Secret name.\n        secret_data: Secret data.\n\n    Returns:\n        True if successful, False otherwise.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/kubernetes/","title":"Kubernetes API","text":""},{"location":"api/kubernetes/#orchestrator","title":"Orchestrator","text":""},{"location":"api/kubernetes/#src.kubernetes.orchestrator","title":"<code>src.kubernetes.orchestrator</code>","text":"<p>Kubernetes Orchestrator for multi-cloud Kubernetes infrastructure.</p>"},{"location":"api/kubernetes/#src.kubernetes.orchestrator.KubernetesOrchestrator","title":"<code>KubernetesOrchestrator</code>","text":"<p>Kubernetes Orchestrator for multi-cloud infrastructure.</p> Source code in <code>src/kubernetes/orchestrator.py</code> <pre><code>class KubernetesOrchestrator:\n    \"\"\"Kubernetes Orchestrator for multi-cloud infrastructure.\"\"\"\n\n    def __init__(self, config_path: str):\n        \"\"\"Initialize Kubernetes Orchestrator.\n\n        Args:\n            config_path: Path to configuration file.\n        \"\"\"\n        self.config_path = config_path\n        self.config = self._load_config(config_path)\n        self.logger = logging.getLogger(f\"{__name__}.KubernetesOrchestrator\")\n\n        # Initialize cloud providers\n        self.cloud_providers = CloudProviderFactory.create_providers(self.config)\n\n        # Initialize components\n        self.api_gateway = None\n        self.gpu_autoscaler = None\n        self.secret_manager = None\n\n        # State\n        self.running = False\n        self.status_thread = None\n        self.status = {\"cloud_providers\": {}, \"api_gateway\": {}, \"autoscaling\": {}, \"secret_manager\": {}}\n\n    def _load_config(self, config_path: str) -&gt; Dict[str, Any]:\n        \"\"\"Load configuration from file.\n\n        Args:\n            config_path: Path to configuration file.\n\n        Returns:\n            Configuration dictionary.\n        \"\"\"\n        with open(config_path, \"r\") as f:\n            if config_path.endswith(\".yaml\") or config_path.endswith(\".yml\"):\n                return yaml.safe_load(f)\n            elif config_path.endswith(\".json\"):\n                return json.load(f)\n            else:\n                raise ValueError(f\"Unsupported configuration file format: {config_path}\")\n\n    def initialize(self) -&gt; bool:\n        \"\"\"Initialize the Kubernetes Orchestrator.\n\n        Returns:\n            True if successful, False otherwise.\n        \"\"\"\n        try:\n            # Initialize API Gateway\n            api_gateway_config = self.config.get(\"api_gateway\", {})\n            self.api_gateway = APIGateway(api_gateway_config, self.cloud_providers)\n\n            # Initialize GPU Autoscaler\n            autoscaling_config = self.config.get(\"autoscaling\", {})\n            self.gpu_autoscaler = GPUAutoscaler(autoscaling_config, self.cloud_providers)\n\n            # Initialize Secret Manager\n            secrets_config = self.config.get(\"secrets\", {})\n            self.secret_manager = SecretManager(secrets_config, self.cloud_providers)\n\n            # Update status\n            self.status[\"cloud_providers\"] = {\n                name: {\"enabled\": provider.is_enabled(), \"region\": provider.region}\n                for name, provider in self.cloud_providers.items()\n            }\n\n            return True\n        except Exception as e:\n            self.logger.error(f\"Error initializing Kubernetes Orchestrator: {e}\")\n            return False\n\n    def start(self) -&gt; bool:\n        \"\"\"Start the Kubernetes Orchestrator.\n\n        Returns:\n            True if successful, False otherwise.\n        \"\"\"\n        if self.running:\n            self.logger.warning(\"Kubernetes Orchestrator is already running\")\n            return True\n\n        try:\n            # Start API Gateway\n            if self.api_gateway:\n                self.api_gateway.setup_global_load_balancer()\n                self.api_gateway.configure_security()\n                self.api_gateway.deploy()\n\n            # Start GPU Autoscaler\n            if self.gpu_autoscaler:\n                self.gpu_autoscaler.start()\n\n            # Start Secret Manager\n            if self.secret_manager:\n                self.secret_manager.start()\n\n            # Start status thread\n            self.running = True\n            self.status_thread = threading.Thread(target=self._status_loop)\n            self.status_thread.daemon = True\n            self.status_thread.start()\n\n            self.logger.info(\"Kubernetes Orchestrator started\")\n            return True\n        except Exception as e:\n            self.logger.error(f\"Error starting Kubernetes Orchestrator: {e}\")\n            return False\n\n    def stop(self) -&gt; bool:\n        \"\"\"Stop the Kubernetes Orchestrator.\n\n        Returns:\n            True if successful, False otherwise.\n        \"\"\"\n        if not self.running:\n            self.logger.warning(\"Kubernetes Orchestrator is not running\")\n            return True\n\n        try:\n            self.running = False\n\n            # Stop status thread\n            if self.status_thread:\n                self.status_thread.join(timeout=5.0)\n                self.status_thread = None\n\n            # Stop GPU Autoscaler\n            if self.gpu_autoscaler:\n                self.gpu_autoscaler.stop()\n\n            # Stop Secret Manager\n            if self.secret_manager:\n                self.secret_manager.stop()\n\n            self.logger.info(\"Kubernetes Orchestrator stopped\")\n            return True\n        except Exception as e:\n            self.logger.error(f\"Error stopping Kubernetes Orchestrator: {e}\")\n            return False\n\n    def _status_loop(self) -&gt; None:\n        \"\"\"Status monitoring loop.\"\"\"\n        while self.running:\n            try:\n                self._update_status()\n            except Exception as e:\n                self.logger.error(f\"Error in status loop: {e}\")\n\n            time.sleep(30)  # Update status every 30 seconds\n\n    def _update_status(self) -&gt; None:\n        \"\"\"Update orchestrator status.\"\"\"\n        # Update API Gateway status\n        if self.api_gateway:\n            self.status[\"api_gateway\"] = {\"routing\": self.api_gateway.get_routing_info()}\n\n        # Update GPU Autoscaler status\n        if self.gpu_autoscaler:\n            self.status[\"autoscaling\"] = {\n                \"metrics\": self.gpu_autoscaler.get_current_metrics(),\n                \"scaling_history\": self.gpu_autoscaler.get_scaling_history(),\n            }\n\n        # Update Secret Manager status\n        if self.secret_manager:\n            self.status[\"secret_manager\"] = {\"rotation_status\": self.secret_manager.get_rotation_status()}\n\n    def deploy_llm_model(self, model_config: Dict[str, Any]) -&gt; bool:\n        \"\"\"Deploy an LLM model to the Kubernetes clusters.\n\n        Args:\n            model_config: LLM model configuration.\n\n        Returns:\n            True if successful, False otherwise.\n        \"\"\"\n        model_name = model_config.get(\"name\")\n        if not model_name:\n            self.logger.error(\"Model name is required\")\n            return False\n\n        self.logger.info(f\"Deploying LLM model {model_name}\")\n\n        # Generate Kubernetes manifests for the model\n        manifests = self._generate_model_manifests(model_config)\n\n        # Deploy manifests to all cloud providers\n        success = True\n\n        for provider_name, provider in self.cloud_providers.items():\n            if not provider.is_enabled():\n                continue\n\n            try:\n                # In a real implementation, this would use the Kubernetes client\n                # to deploy the manifests to the cluster\n                self.logger.info(f\"Deploying LLM model {model_name} to {provider_name}\")\n\n                # Simulated success for demonstration\n                model_endpoint = f\"https://{model_name}.{provider_name}.example.com\"\n\n                # Register model endpoint in API Gateway\n                if self.api_gateway:\n                    self.api_gateway.register_route(\n                        path=f\"/api/models/{model_name}\",\n                        method=\"POST\",\n                        service_name=f\"{model_name}-service\",\n                        service_port=8000,\n                    )\n\n                self.logger.info(f\"Successfully deployed LLM model {model_name} to {provider_name}\")\n            except Exception as e:\n                self.logger.error(f\"Error deploying LLM model {model_name} to {provider_name}: {e}\")\n                success = False\n\n        return success\n\n    def _generate_model_manifests(self, model_config: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Generate Kubernetes manifests for an LLM model.\n\n        Args:\n            model_config: LLM model configuration.\n\n        Returns:\n            Dictionary of Kubernetes manifests.\n        \"\"\"\n        model_name = model_config.get(\"name\")\n        version = model_config.get(\"version\", \"1.0\")\n        resource_requirements = model_config.get(\"resource_requirements\", {})\n        container_config = model_config.get(\"container\", {})\n\n        # Generate manifests\n        deployment = {\n            \"apiVersion\": \"apps/v1\",\n            \"kind\": \"Deployment\",\n            \"metadata\": {\n                \"name\": f\"{model_name}-deployment\",\n                \"namespace\": self.config.get(\"kubernetes\", {}).get(\"namespace\", \"llm-serving\"),\n                \"labels\": {\"app\": model_name, \"version\": version},\n            },\n            \"spec\": {\n                \"replicas\": 1,  # Initial replica count, will be scaled by autoscaler\n                \"selector\": {\"matchLabels\": {\"app\": model_name}},\n                \"template\": {\n                    \"metadata\": {\"labels\": {\"app\": model_name, \"version\": version}},\n                    \"spec\": {\n                        \"containers\": [\n                            {\n                                \"name\": model_name,\n                                \"image\": container_config.get(\"image\"),\n                                \"ports\": [{\"containerPort\": container_config.get(\"port\", 8000)}],\n                                \"resources\": {\n                                    \"requests\": {\n                                        \"cpu\": f\"{resource_requirements.get('cpu', 1)}\",\n                                        \"memory\": resource_requirements.get(\"memory\", \"1Gi\"),\n                                        \"nvidia.com/gpu\": resource_requirements.get(\"gpu\", 0),\n                                    },\n                                    \"limits\": {\n                                        \"cpu\": f\"{resource_requirements.get('cpu', 1) * 2}\",\n                                        \"memory\": resource_requirements.get(\"memory\", \"1Gi\"),\n                                        \"nvidia.com/gpu\": resource_requirements.get(\"gpu\", 0),\n                                    },\n                                },\n                                \"env\": [\n                                    {\"name\": name, \"value\": value}\n                                    for name, value in container_config.get(\"environment\", {}).items()\n                                ],\n                                \"volumeMounts\": [\n                                    {\"name\": mount.get(\"name\"), \"mountPath\": mount.get(\"mount_path\")}\n                                    for mount in container_config.get(\"volume_mounts\", [])\n                                ],\n                            }\n                        ],\n                        \"nodeSelector\": {\"node-type\": \"gpu\" if resource_requirements.get(\"gpu\", 0) &gt; 0 else \"cpu\"},\n                        \"volumes\": [{\"name\": \"models-volume\", \"persistentVolumeClaim\": {\"claimName\": \"models-pvc\"}}],\n                    },\n                },\n            },\n        }\n\n        service = {\n            \"apiVersion\": \"v1\",\n            \"kind\": \"Service\",\n            \"metadata\": {\n                \"name\": f\"{model_name}-service\",\n                \"namespace\": self.config.get(\"kubernetes\", {}).get(\"namespace\", \"llm-serving\"),\n            },\n            \"spec\": {\n                \"selector\": {\"app\": model_name},\n                \"ports\": [{\"port\": 80, \"targetPort\": container_config.get(\"port\", 8000)}],\n            },\n        }\n\n        # If Istio is enabled, add VirtualService\n        if self.config.get(\"kubernetes\", {}).get(\"istio\", {}).get(\"enabled\", False):\n            virtual_service = {\n                \"apiVersion\": \"networking.istio.io/v1alpha3\",\n                \"kind\": \"VirtualService\",\n                \"metadata\": {\n                    \"name\": f\"{model_name}-vs\",\n                    \"namespace\": self.config.get(\"kubernetes\", {}).get(\"namespace\", \"llm-serving\"),\n                },\n                \"spec\": {\n                    \"hosts\": [f\"{model_name}.example.com\"],\n                    \"gateways\": [\n                        self.config.get(\"kubernetes\", {}).get(\"istio\", {}).get(\"gateway\", {}).get(\"name\", \"llm-gateway\")\n                    ],\n                    \"http\": [{\"route\": [{\"destination\": {\"host\": f\"{model_name}-service\", \"port\": {\"number\": 80}}}]}],\n                },\n            }\n\n            return {\"deployment\": deployment, \"service\": service, \"virtual_service\": virtual_service}\n        else:\n            return {\"deployment\": deployment, \"service\": service}\n\n    def handle_cloud_failure(self, failed_provider: str) -&gt; bool:\n        \"\"\"Handle failure of a cloud provider.\n\n        Args:\n            failed_provider: Failed cloud provider name.\n\n        Returns:\n            True if successfully handled, False otherwise.\n        \"\"\"\n        self.logger.warning(f\"Handling failure of cloud provider {failed_provider}\")\n\n        try:\n            # Update API Gateway routing to avoid the failed provider\n            if self.api_gateway:\n                # In a real implementation, this would update the global load balancer\n                # to avoid routing traffic to the failed provider\n                self.logger.info(f\"Updating API Gateway routing to avoid {failed_provider}\")\n\n            # Scale up resources in other providers to handle the load\n            for provider_name, provider in self.cloud_providers.items():\n                if provider_name == failed_provider or not provider.is_enabled():\n                    continue\n\n                self.logger.info(f\"Scaling up resources in {provider_name} to handle load from {failed_provider}\")\n\n                # In a real implementation, this would scale up resources in the other providers\n\n            self.logger.info(f\"Successfully handled failure of cloud provider {failed_provider}\")\n            return True\n        except Exception as e:\n            self.logger.error(f\"Error handling failure of cloud provider {failed_provider}: {e}\")\n            return False\n\n    def get_status(self) -&gt; Dict[str, Any]:\n        \"\"\"Get orchestrator status.\n\n        Returns:\n            Dictionary of orchestrator status.\n        \"\"\"\n        # Return a copy of the status\n        return self.status.copy()\n\n    def monitor_costs(self) -&gt; Dict[str, float]:\n        \"\"\"Monitor costs across all cloud providers.\n\n        Returns:\n            Dictionary of costs per provider.\n        \"\"\"\n        costs = {}\n\n        for provider_name, provider in self.cloud_providers.items():\n            if not provider.is_enabled():\n                continue\n\n            try:\n                cost_metrics = provider.get_cost_metrics(timeframe=\"daily\")\n                costs[provider_name] = cost_metrics.get(\"total_cost\", 0.0)\n            except Exception as e:\n                self.logger.error(f\"Error getting cost metrics for {provider_name}: {e}\")\n                costs[provider_name] = 0.0\n\n        return costs\n\n    def optimize_costs(self) -&gt; bool:\n        \"\"\"Optimize costs across all cloud providers.\n\n        Returns:\n            True if successful, False otherwise.\n        \"\"\"\n        cost_optimization_config = self.config.get(\"cost_management\", {}).get(\"optimization\", {})\n        enable_spot_instances = cost_optimization_config.get(\"enable_spot_instances\", True)\n\n        self.logger.info(f\"Optimizing costs with spot instances {'enabled' if enable_spot_instances else 'disabled'}\")\n\n        # In a real implementation, this would optimize costs across cloud providers\n        # For example, by using spot/preemptible instances, right-sizing resources, etc.\n\n        # For simplicity, we'll just return success\n        return True\n\n    def sync_configurations(self) -&gt; bool:\n        \"\"\"Synchronize configurations across all cloud providers.\n\n        Returns:\n            True if successful, False otherwise.\n        \"\"\"\n        self.logger.info(\"Synchronizing configurations across all cloud providers\")\n\n        # In a real implementation, this would ensure that configurations are synchronized\n        # across all cloud providers\n\n        # For simplicity, we'll just return success\n        return True\n\n    def deploy_monitoring(self) -&gt; bool:\n        \"\"\"Deploy monitoring infrastructure across all cloud providers.\n\n        Returns:\n            True if successful, False otherwise.\n        \"\"\"\n        monitoring_config = self.config.get(\"monitoring\", {})\n        prometheus_enabled = monitoring_config.get(\"prometheus\", {}).get(\"enabled\", True)\n        grafana_enabled = monitoring_config.get(\"grafana\", {}).get(\"enabled\", True)\n\n        self.logger.info(\n            f\"Deploying monitoring infrastructure with Prometheus {'enabled' if prometheus_enabled else 'disabled'} and Grafana {'enabled' if grafana_enabled else 'disabled'}\"\n        )\n\n        # In a real implementation, this would deploy monitoring infrastructure\n        # such as Prometheus, Grafana, and alerts\n\n        # For simplicity, we'll just return success\n        return True\n\n    def get_deployment_details(self, model_name: str) -&gt; Dict[str, Any]:\n        \"\"\"Get deployment details for a specific model.\n\n        Args:\n            model_name: Model name.\n\n        Returns:\n            Dictionary of deployment details.\n        \"\"\"\n        details = {}\n\n        for provider_name, provider in self.cloud_providers.items():\n            if not provider.is_enabled():\n                continue\n\n            # In a real implementation, this would get deployment details\n            # from the Kubernetes cluster\n\n            # Simulated details for demonstration\n            details[provider_name] = {\n                \"status\": \"Running\",\n                \"replicas\": 3,\n                \"endpoint\": f\"https://{model_name}.{provider_name}.example.com\",\n                \"created_at\": \"2023-01-01T00:00:00Z\",\n                \"updated_at\": \"2023-01-01T00:00:00Z\",\n            }\n\n        return details\n</code></pre>"},{"location":"api/kubernetes/#src.kubernetes.orchestrator.KubernetesOrchestrator.__init__","title":"<code>__init__(config_path)</code>","text":"<p>Initialize Kubernetes Orchestrator.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str</code> <p>Path to configuration file.</p> required Source code in <code>src/kubernetes/orchestrator.py</code> <pre><code>def __init__(self, config_path: str):\n    \"\"\"Initialize Kubernetes Orchestrator.\n\n    Args:\n        config_path: Path to configuration file.\n    \"\"\"\n    self.config_path = config_path\n    self.config = self._load_config(config_path)\n    self.logger = logging.getLogger(f\"{__name__}.KubernetesOrchestrator\")\n\n    # Initialize cloud providers\n    self.cloud_providers = CloudProviderFactory.create_providers(self.config)\n\n    # Initialize components\n    self.api_gateway = None\n    self.gpu_autoscaler = None\n    self.secret_manager = None\n\n    # State\n    self.running = False\n    self.status_thread = None\n    self.status = {\"cloud_providers\": {}, \"api_gateway\": {}, \"autoscaling\": {}, \"secret_manager\": {}}\n</code></pre>"},{"location":"api/kubernetes/#src.kubernetes.orchestrator.KubernetesOrchestrator.deploy_llm_model","title":"<code>deploy_llm_model(model_config)</code>","text":"<p>Deploy an LLM model to the Kubernetes clusters.</p> <p>Parameters:</p> Name Type Description Default <code>model_config</code> <code>Dict[str, Any]</code> <p>LLM model configuration.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if successful, False otherwise.</p> Source code in <code>src/kubernetes/orchestrator.py</code> <pre><code>def deploy_llm_model(self, model_config: Dict[str, Any]) -&gt; bool:\n    \"\"\"Deploy an LLM model to the Kubernetes clusters.\n\n    Args:\n        model_config: LLM model configuration.\n\n    Returns:\n        True if successful, False otherwise.\n    \"\"\"\n    model_name = model_config.get(\"name\")\n    if not model_name:\n        self.logger.error(\"Model name is required\")\n        return False\n\n    self.logger.info(f\"Deploying LLM model {model_name}\")\n\n    # Generate Kubernetes manifests for the model\n    manifests = self._generate_model_manifests(model_config)\n\n    # Deploy manifests to all cloud providers\n    success = True\n\n    for provider_name, provider in self.cloud_providers.items():\n        if not provider.is_enabled():\n            continue\n\n        try:\n            # In a real implementation, this would use the Kubernetes client\n            # to deploy the manifests to the cluster\n            self.logger.info(f\"Deploying LLM model {model_name} to {provider_name}\")\n\n            # Simulated success for demonstration\n            model_endpoint = f\"https://{model_name}.{provider_name}.example.com\"\n\n            # Register model endpoint in API Gateway\n            if self.api_gateway:\n                self.api_gateway.register_route(\n                    path=f\"/api/models/{model_name}\",\n                    method=\"POST\",\n                    service_name=f\"{model_name}-service\",\n                    service_port=8000,\n                )\n\n            self.logger.info(f\"Successfully deployed LLM model {model_name} to {provider_name}\")\n        except Exception as e:\n            self.logger.error(f\"Error deploying LLM model {model_name} to {provider_name}: {e}\")\n            success = False\n\n    return success\n</code></pre>"},{"location":"api/kubernetes/#src.kubernetes.orchestrator.KubernetesOrchestrator.deploy_monitoring","title":"<code>deploy_monitoring()</code>","text":"<p>Deploy monitoring infrastructure across all cloud providers.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if successful, False otherwise.</p> Source code in <code>src/kubernetes/orchestrator.py</code> <pre><code>def deploy_monitoring(self) -&gt; bool:\n    \"\"\"Deploy monitoring infrastructure across all cloud providers.\n\n    Returns:\n        True if successful, False otherwise.\n    \"\"\"\n    monitoring_config = self.config.get(\"monitoring\", {})\n    prometheus_enabled = monitoring_config.get(\"prometheus\", {}).get(\"enabled\", True)\n    grafana_enabled = monitoring_config.get(\"grafana\", {}).get(\"enabled\", True)\n\n    self.logger.info(\n        f\"Deploying monitoring infrastructure with Prometheus {'enabled' if prometheus_enabled else 'disabled'} and Grafana {'enabled' if grafana_enabled else 'disabled'}\"\n    )\n\n    # In a real implementation, this would deploy monitoring infrastructure\n    # such as Prometheus, Grafana, and alerts\n\n    # For simplicity, we'll just return success\n    return True\n</code></pre>"},{"location":"api/kubernetes/#src.kubernetes.orchestrator.KubernetesOrchestrator.get_deployment_details","title":"<code>get_deployment_details(model_name)</code>","text":"<p>Get deployment details for a specific model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Model name.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary of deployment details.</p> Source code in <code>src/kubernetes/orchestrator.py</code> <pre><code>def get_deployment_details(self, model_name: str) -&gt; Dict[str, Any]:\n    \"\"\"Get deployment details for a specific model.\n\n    Args:\n        model_name: Model name.\n\n    Returns:\n        Dictionary of deployment details.\n    \"\"\"\n    details = {}\n\n    for provider_name, provider in self.cloud_providers.items():\n        if not provider.is_enabled():\n            continue\n\n        # In a real implementation, this would get deployment details\n        # from the Kubernetes cluster\n\n        # Simulated details for demonstration\n        details[provider_name] = {\n            \"status\": \"Running\",\n            \"replicas\": 3,\n            \"endpoint\": f\"https://{model_name}.{provider_name}.example.com\",\n            \"created_at\": \"2023-01-01T00:00:00Z\",\n            \"updated_at\": \"2023-01-01T00:00:00Z\",\n        }\n\n    return details\n</code></pre>"},{"location":"api/kubernetes/#src.kubernetes.orchestrator.KubernetesOrchestrator.get_status","title":"<code>get_status()</code>","text":"<p>Get orchestrator status.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary of orchestrator status.</p> Source code in <code>src/kubernetes/orchestrator.py</code> <pre><code>def get_status(self) -&gt; Dict[str, Any]:\n    \"\"\"Get orchestrator status.\n\n    Returns:\n        Dictionary of orchestrator status.\n    \"\"\"\n    # Return a copy of the status\n    return self.status.copy()\n</code></pre>"},{"location":"api/kubernetes/#src.kubernetes.orchestrator.KubernetesOrchestrator.handle_cloud_failure","title":"<code>handle_cloud_failure(failed_provider)</code>","text":"<p>Handle failure of a cloud provider.</p> <p>Parameters:</p> Name Type Description Default <code>failed_provider</code> <code>str</code> <p>Failed cloud provider name.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if successfully handled, False otherwise.</p> Source code in <code>src/kubernetes/orchestrator.py</code> <pre><code>def handle_cloud_failure(self, failed_provider: str) -&gt; bool:\n    \"\"\"Handle failure of a cloud provider.\n\n    Args:\n        failed_provider: Failed cloud provider name.\n\n    Returns:\n        True if successfully handled, False otherwise.\n    \"\"\"\n    self.logger.warning(f\"Handling failure of cloud provider {failed_provider}\")\n\n    try:\n        # Update API Gateway routing to avoid the failed provider\n        if self.api_gateway:\n            # In a real implementation, this would update the global load balancer\n            # to avoid routing traffic to the failed provider\n            self.logger.info(f\"Updating API Gateway routing to avoid {failed_provider}\")\n\n        # Scale up resources in other providers to handle the load\n        for provider_name, provider in self.cloud_providers.items():\n            if provider_name == failed_provider or not provider.is_enabled():\n                continue\n\n            self.logger.info(f\"Scaling up resources in {provider_name} to handle load from {failed_provider}\")\n\n            # In a real implementation, this would scale up resources in the other providers\n\n        self.logger.info(f\"Successfully handled failure of cloud provider {failed_provider}\")\n        return True\n    except Exception as e:\n        self.logger.error(f\"Error handling failure of cloud provider {failed_provider}: {e}\")\n        return False\n</code></pre>"},{"location":"api/kubernetes/#src.kubernetes.orchestrator.KubernetesOrchestrator.initialize","title":"<code>initialize()</code>","text":"<p>Initialize the Kubernetes Orchestrator.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if successful, False otherwise.</p> Source code in <code>src/kubernetes/orchestrator.py</code> <pre><code>def initialize(self) -&gt; bool:\n    \"\"\"Initialize the Kubernetes Orchestrator.\n\n    Returns:\n        True if successful, False otherwise.\n    \"\"\"\n    try:\n        # Initialize API Gateway\n        api_gateway_config = self.config.get(\"api_gateway\", {})\n        self.api_gateway = APIGateway(api_gateway_config, self.cloud_providers)\n\n        # Initialize GPU Autoscaler\n        autoscaling_config = self.config.get(\"autoscaling\", {})\n        self.gpu_autoscaler = GPUAutoscaler(autoscaling_config, self.cloud_providers)\n\n        # Initialize Secret Manager\n        secrets_config = self.config.get(\"secrets\", {})\n        self.secret_manager = SecretManager(secrets_config, self.cloud_providers)\n\n        # Update status\n        self.status[\"cloud_providers\"] = {\n            name: {\"enabled\": provider.is_enabled(), \"region\": provider.region}\n            for name, provider in self.cloud_providers.items()\n        }\n\n        return True\n    except Exception as e:\n        self.logger.error(f\"Error initializing Kubernetes Orchestrator: {e}\")\n        return False\n</code></pre>"},{"location":"api/kubernetes/#src.kubernetes.orchestrator.KubernetesOrchestrator.monitor_costs","title":"<code>monitor_costs()</code>","text":"<p>Monitor costs across all cloud providers.</p> <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dictionary of costs per provider.</p> Source code in <code>src/kubernetes/orchestrator.py</code> <pre><code>def monitor_costs(self) -&gt; Dict[str, float]:\n    \"\"\"Monitor costs across all cloud providers.\n\n    Returns:\n        Dictionary of costs per provider.\n    \"\"\"\n    costs = {}\n\n    for provider_name, provider in self.cloud_providers.items():\n        if not provider.is_enabled():\n            continue\n\n        try:\n            cost_metrics = provider.get_cost_metrics(timeframe=\"daily\")\n            costs[provider_name] = cost_metrics.get(\"total_cost\", 0.0)\n        except Exception as e:\n            self.logger.error(f\"Error getting cost metrics for {provider_name}: {e}\")\n            costs[provider_name] = 0.0\n\n    return costs\n</code></pre>"},{"location":"api/kubernetes/#src.kubernetes.orchestrator.KubernetesOrchestrator.optimize_costs","title":"<code>optimize_costs()</code>","text":"<p>Optimize costs across all cloud providers.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if successful, False otherwise.</p> Source code in <code>src/kubernetes/orchestrator.py</code> <pre><code>def optimize_costs(self) -&gt; bool:\n    \"\"\"Optimize costs across all cloud providers.\n\n    Returns:\n        True if successful, False otherwise.\n    \"\"\"\n    cost_optimization_config = self.config.get(\"cost_management\", {}).get(\"optimization\", {})\n    enable_spot_instances = cost_optimization_config.get(\"enable_spot_instances\", True)\n\n    self.logger.info(f\"Optimizing costs with spot instances {'enabled' if enable_spot_instances else 'disabled'}\")\n\n    # In a real implementation, this would optimize costs across cloud providers\n    # For example, by using spot/preemptible instances, right-sizing resources, etc.\n\n    # For simplicity, we'll just return success\n    return True\n</code></pre>"},{"location":"api/kubernetes/#src.kubernetes.orchestrator.KubernetesOrchestrator.start","title":"<code>start()</code>","text":"<p>Start the Kubernetes Orchestrator.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if successful, False otherwise.</p> Source code in <code>src/kubernetes/orchestrator.py</code> <pre><code>def start(self) -&gt; bool:\n    \"\"\"Start the Kubernetes Orchestrator.\n\n    Returns:\n        True if successful, False otherwise.\n    \"\"\"\n    if self.running:\n        self.logger.warning(\"Kubernetes Orchestrator is already running\")\n        return True\n\n    try:\n        # Start API Gateway\n        if self.api_gateway:\n            self.api_gateway.setup_global_load_balancer()\n            self.api_gateway.configure_security()\n            self.api_gateway.deploy()\n\n        # Start GPU Autoscaler\n        if self.gpu_autoscaler:\n            self.gpu_autoscaler.start()\n\n        # Start Secret Manager\n        if self.secret_manager:\n            self.secret_manager.start()\n\n        # Start status thread\n        self.running = True\n        self.status_thread = threading.Thread(target=self._status_loop)\n        self.status_thread.daemon = True\n        self.status_thread.start()\n\n        self.logger.info(\"Kubernetes Orchestrator started\")\n        return True\n    except Exception as e:\n        self.logger.error(f\"Error starting Kubernetes Orchestrator: {e}\")\n        return False\n</code></pre>"},{"location":"api/kubernetes/#src.kubernetes.orchestrator.KubernetesOrchestrator.stop","title":"<code>stop()</code>","text":"<p>Stop the Kubernetes Orchestrator.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if successful, False otherwise.</p> Source code in <code>src/kubernetes/orchestrator.py</code> <pre><code>def stop(self) -&gt; bool:\n    \"\"\"Stop the Kubernetes Orchestrator.\n\n    Returns:\n        True if successful, False otherwise.\n    \"\"\"\n    if not self.running:\n        self.logger.warning(\"Kubernetes Orchestrator is not running\")\n        return True\n\n    try:\n        self.running = False\n\n        # Stop status thread\n        if self.status_thread:\n            self.status_thread.join(timeout=5.0)\n            self.status_thread = None\n\n        # Stop GPU Autoscaler\n        if self.gpu_autoscaler:\n            self.gpu_autoscaler.stop()\n\n        # Stop Secret Manager\n        if self.secret_manager:\n            self.secret_manager.stop()\n\n        self.logger.info(\"Kubernetes Orchestrator stopped\")\n        return True\n    except Exception as e:\n        self.logger.error(f\"Error stopping Kubernetes Orchestrator: {e}\")\n        return False\n</code></pre>"},{"location":"api/kubernetes/#src.kubernetes.orchestrator.KubernetesOrchestrator.sync_configurations","title":"<code>sync_configurations()</code>","text":"<p>Synchronize configurations across all cloud providers.</p> <p>Returns:</p> Type Description <code>bool</code> <p>True if successful, False otherwise.</p> Source code in <code>src/kubernetes/orchestrator.py</code> <pre><code>def sync_configurations(self) -&gt; bool:\n    \"\"\"Synchronize configurations across all cloud providers.\n\n    Returns:\n        True if successful, False otherwise.\n    \"\"\"\n    self.logger.info(\"Synchronizing configurations across all cloud providers\")\n\n    # In a real implementation, this would ensure that configurations are synchronized\n    # across all cloud providers\n\n    # For simplicity, we'll just return success\n    return True\n</code></pre>"},{"location":"api/models/","title":"Models API","text":""},{"location":"api/models/#modelbase","title":"ModelBase","text":""},{"location":"api/models/#src.models.model_base.ModelBase","title":"<code>src.models.model_base.ModelBase</code>","text":"<p>Base class for all ML models.</p> Source code in <code>src/models/model_base.py</code> <pre><code>class ModelBase:\n    \"\"\"Base class for all ML models.\"\"\"\n\n    def __init__(self, model_name: str, models_dir: Optional[str] = None):\n        \"\"\"Initialize model.\n\n        Args:\n            model_name: Name of the model.\n            models_dir: Directory to save/load models. If None, use current directory.\n        \"\"\"\n        self.model_name = model_name\n        self.models_dir = Path(models_dir) if models_dir else Path.cwd() / \"models\"\n        self.model = None\n        self.metadata = {\n            \"name\": model_name,\n            \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n            \"params\": {},\n            \"metrics\": {},\n        }\n        os.makedirs(self.models_dir, exist_ok=True)\n\n    def train(self, X: Union[np.ndarray, pd.DataFrame], y: Union[np.ndarray, pd.Series], **kwargs) -&gt; Dict[str, Any]:\n        \"\"\"Train the model.\n\n        Args:\n            X: Training features.\n            y: Training labels.\n            **kwargs: Additional training parameters.\n\n        Returns:\n            Dictionary containing training metrics.\n        \"\"\"\n        raise NotImplementedError(\"Subclasses must implement train()\")\n\n    def predict(self, X: Union[np.ndarray, pd.DataFrame]) -&gt; np.ndarray:\n        \"\"\"Make predictions using the model.\n\n        Args:\n            X: Features to predict.\n\n        Returns:\n            Predicted values.\n        \"\"\"\n        if self.model is None:\n            raise ValueError(\"Model not trained or loaded. Call train() or load() first.\")\n        raise NotImplementedError(\"Subclasses must implement predict()\")\n\n    def evaluate(self, X: Union[np.ndarray, pd.DataFrame], y: Union[np.ndarray, pd.Series]) -&gt; Dict[str, float]:\n        \"\"\"Evaluate the model.\n\n        Args:\n            X: Evaluation features.\n            y: Evaluation labels.\n\n        Returns:\n            Dictionary containing evaluation metrics.\n        \"\"\"\n        if self.model is None:\n            raise ValueError(\"Model not trained or loaded. Call train() or load() first.\")\n        raise NotImplementedError(\"Subclasses must implement evaluate()\")\n\n    def save(\n        self,\n        filename: Optional[str] = None,\n        secure: bool = False,\n        secure_weights_manager=None,\n        version: Optional[str] = None,\n    ) -&gt; str:\n        \"\"\"Save model to disk.\n\n        Args:\n            filename: Filename to save model. If None, use model_name.\n            secure: Whether to save with secure weights storage.\n            secure_weights_manager: SecureModelWeights instance for secure storage.\n            version: Version of the model weights. If None, a timestamp-based version is used.\n\n        Returns:\n            Path to saved model or a dictionary with secure storage details.\n        \"\"\"\n        if self.model is None:\n            raise ValueError(\"No model to save. Train a model first.\")\n\n        # If secure storage is requested but no manager is provided\n        if secure and secure_weights_manager is None:\n            logger.warning(\n                \"Secure storage requested but no secure_weights_manager provided. Falling back to standard storage.\"\n            )\n            secure = False\n\n        # Standard storage option\n        if not secure:\n            if filename is None:\n                filename = f\"{self.model_name}.pkl\"\n\n            file_path = self.models_dir / filename\n            logger.info(f\"Saving model to {file_path}\")\n\n            try:\n                with open(file_path, \"wb\") as f:\n                    pickle.dump({\"model\": self.model, \"metadata\": self.metadata}, f)\n                return str(file_path)\n            except Exception as e:\n                logger.error(f\"Error saving model to {file_path}: {e}\")\n                raise\n\n        # Secure storage option\n        else:\n            try:\n                logger.info(f\"Saving model {self.model_name} with secure storage\")\n\n                # Create a temporary file for the model\n                with tempfile.NamedTemporaryFile(suffix=\".pkl\", delete=False) as temp_file:\n                    temp_path = temp_file.name\n\n                    # Save model to temporary file\n                    pickle.dump({\"model\": self.model, \"metadata\": self.metadata}, temp_file)\n\n                # Store in secure weights manager\n                additional_metadata = {\n                    \"model_type\": self.__class__.__name__,\n                    \"params\": self.metadata.get(\"params\", {}),\n                    \"metrics\": self.metadata.get(\"metrics\", {}),\n                }\n\n                result = secure_weights_manager.store_weights(\n                    model_name=self.model_name,\n                    weights_file=temp_path,\n                    version=version,\n                    metadata=additional_metadata,\n                    encrypt=True,\n                )\n\n                # Delete the temporary file\n                try:\n                    os.unlink(temp_path)\n                except OSError:\n                    pass\n\n                # Return information about the stored model\n                return result\n\n            except Exception as e:\n                logger.error(f\"Error saving model {self.model_name} to secure storage: {e}\")\n                raise\n\n    def load(\n        self,\n        filename: Optional[str] = None,\n        secure: bool = False,\n        secure_weights_manager=None,\n        version: Optional[str] = None,\n    ) -&gt; None:\n        \"\"\"Load model from disk.\n\n        Args:\n            filename: Filename to load model from. If None, use model_name.\n            secure: Whether to load from secure weights storage.\n            secure_weights_manager: SecureModelWeights instance for secure storage.\n            version: Version of the model weights to load. If None, the latest version is used.\n        \"\"\"\n        # If secure storage is requested but no manager is provided\n        if secure and secure_weights_manager is None:\n            logger.warning(\n                \"Secure storage requested but no secure_weights_manager provided. Falling back to standard storage.\"\n            )\n            secure = False\n\n        # Standard storage option\n        if not secure:\n            if filename is None:\n                filename = f\"{self.model_name}.pkl\"\n\n            file_path = self.models_dir / filename\n            logger.info(f\"Loading model from {file_path}\")\n\n            try:\n                with open(file_path, \"rb\") as f:\n                    data = pickle.load(f)\n                    self.model = data[\"model\"]\n                    self.metadata = data[\"metadata\"]\n            except Exception as e:\n                logger.error(f\"Error loading model from {file_path}: {e}\")\n                raise\n\n        # Secure storage option\n        else:\n            try:\n                logger.info(f\"Loading model {self.model_name} from secure storage (version: {version or 'latest'})\")\n\n                # Create a temporary file to store the model\n                with tempfile.NamedTemporaryFile(suffix=\".pkl\", delete=False) as temp_file:\n                    temp_path = temp_file.name\n\n                # Load weights to temporary file\n                weights_metadata = secure_weights_manager.load_weights_to_file(\n                    model_name=self.model_name, output_path=temp_path, version=version, decrypt=True\n                )\n\n                # Load model from temporary file\n                with open(temp_path, \"rb\") as f:\n                    data = pickle.load(f)\n                    self.model = data[\"model\"]\n                    self.metadata = data[\"metadata\"]\n\n                # Update metadata with any additional info from secure storage\n                if \"params\" in weights_metadata:\n                    self.metadata[\"params\"].update(weights_metadata[\"params\"])\n\n                if \"metrics\" in weights_metadata:\n                    self.metadata[\"metrics\"].update(weights_metadata[\"metrics\"])\n\n                # Delete the temporary file\n                try:\n                    os.unlink(temp_path)\n                except OSError:\n                    pass\n\n                logger.info(\n                    f\"Successfully loaded model {self.model_name} version {weights_metadata.get('version', 'unknown')}\"\n                )\n\n            except Exception as e:\n                logger.error(f\"Error loading model {self.model_name} from secure storage: {e}\")\n                raise\n\n    def get_params(self) -&gt; Dict[str, Any]:\n        \"\"\"Get model parameters.\n\n        Returns:\n            Dictionary containing model parameters.\n        \"\"\"\n        return self.metadata[\"params\"]\n\n    def set_params(self, params: Dict[str, Any]) -&gt; None:\n        \"\"\"Set model parameters.\n\n        Args:\n            params: Dictionary containing model parameters.\n        \"\"\"\n        self.metadata[\"params\"].update(params)\n\n    def log_metric(self, name: str, value: float) -&gt; None:\n        \"\"\"Log a metric.\n\n        Args:\n            name: Metric name.\n            value: Metric value.\n        \"\"\"\n        self.metadata[\"metrics\"][name] = value\n        logger.info(f\"Metric {name}: {value}\")\n\n    def get_metrics(self) -&gt; Dict[str, float]:\n        \"\"\"Get all metrics.\n\n        Returns:\n            Dictionary containing all metrics.\n        \"\"\"\n        return self.metadata[\"metrics\"]\n</code></pre>"},{"location":"api/models/#src.models.model_base.ModelBase.__init__","title":"<code>__init__(model_name, models_dir=None)</code>","text":"<p>Initialize model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model.</p> required <code>models_dir</code> <code>Optional[str]</code> <p>Directory to save/load models. If None, use current directory.</p> <code>None</code> Source code in <code>src/models/model_base.py</code> <pre><code>def __init__(self, model_name: str, models_dir: Optional[str] = None):\n    \"\"\"Initialize model.\n\n    Args:\n        model_name: Name of the model.\n        models_dir: Directory to save/load models. If None, use current directory.\n    \"\"\"\n    self.model_name = model_name\n    self.models_dir = Path(models_dir) if models_dir else Path.cwd() / \"models\"\n    self.model = None\n    self.metadata = {\n        \"name\": model_name,\n        \"created_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n        \"params\": {},\n        \"metrics\": {},\n    }\n    os.makedirs(self.models_dir, exist_ok=True)\n</code></pre>"},{"location":"api/models/#src.models.model_base.ModelBase.evaluate","title":"<code>evaluate(X, y)</code>","text":"<p>Evaluate the model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[ndarray, DataFrame]</code> <p>Evaluation features.</p> required <code>y</code> <code>Union[ndarray, Series]</code> <p>Evaluation labels.</p> required <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dictionary containing evaluation metrics.</p> Source code in <code>src/models/model_base.py</code> <pre><code>def evaluate(self, X: Union[np.ndarray, pd.DataFrame], y: Union[np.ndarray, pd.Series]) -&gt; Dict[str, float]:\n    \"\"\"Evaluate the model.\n\n    Args:\n        X: Evaluation features.\n        y: Evaluation labels.\n\n    Returns:\n        Dictionary containing evaluation metrics.\n    \"\"\"\n    if self.model is None:\n        raise ValueError(\"Model not trained or loaded. Call train() or load() first.\")\n    raise NotImplementedError(\"Subclasses must implement evaluate()\")\n</code></pre>"},{"location":"api/models/#src.models.model_base.ModelBase.get_metrics","title":"<code>get_metrics()</code>","text":"<p>Get all metrics.</p> <p>Returns:</p> Type Description <code>Dict[str, float]</code> <p>Dictionary containing all metrics.</p> Source code in <code>src/models/model_base.py</code> <pre><code>def get_metrics(self) -&gt; Dict[str, float]:\n    \"\"\"Get all metrics.\n\n    Returns:\n        Dictionary containing all metrics.\n    \"\"\"\n    return self.metadata[\"metrics\"]\n</code></pre>"},{"location":"api/models/#src.models.model_base.ModelBase.get_params","title":"<code>get_params()</code>","text":"<p>Get model parameters.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing model parameters.</p> Source code in <code>src/models/model_base.py</code> <pre><code>def get_params(self) -&gt; Dict[str, Any]:\n    \"\"\"Get model parameters.\n\n    Returns:\n        Dictionary containing model parameters.\n    \"\"\"\n    return self.metadata[\"params\"]\n</code></pre>"},{"location":"api/models/#src.models.model_base.ModelBase.load","title":"<code>load(filename=None, secure=False, secure_weights_manager=None, version=None)</code>","text":"<p>Load model from disk.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>Optional[str]</code> <p>Filename to load model from. If None, use model_name.</p> <code>None</code> <code>secure</code> <code>bool</code> <p>Whether to load from secure weights storage.</p> <code>False</code> <code>secure_weights_manager</code> <p>SecureModelWeights instance for secure storage.</p> <code>None</code> <code>version</code> <code>Optional[str]</code> <p>Version of the model weights to load. If None, the latest version is used.</p> <code>None</code> Source code in <code>src/models/model_base.py</code> <pre><code>def load(\n    self,\n    filename: Optional[str] = None,\n    secure: bool = False,\n    secure_weights_manager=None,\n    version: Optional[str] = None,\n) -&gt; None:\n    \"\"\"Load model from disk.\n\n    Args:\n        filename: Filename to load model from. If None, use model_name.\n        secure: Whether to load from secure weights storage.\n        secure_weights_manager: SecureModelWeights instance for secure storage.\n        version: Version of the model weights to load. If None, the latest version is used.\n    \"\"\"\n    # If secure storage is requested but no manager is provided\n    if secure and secure_weights_manager is None:\n        logger.warning(\n            \"Secure storage requested but no secure_weights_manager provided. Falling back to standard storage.\"\n        )\n        secure = False\n\n    # Standard storage option\n    if not secure:\n        if filename is None:\n            filename = f\"{self.model_name}.pkl\"\n\n        file_path = self.models_dir / filename\n        logger.info(f\"Loading model from {file_path}\")\n\n        try:\n            with open(file_path, \"rb\") as f:\n                data = pickle.load(f)\n                self.model = data[\"model\"]\n                self.metadata = data[\"metadata\"]\n        except Exception as e:\n            logger.error(f\"Error loading model from {file_path}: {e}\")\n            raise\n\n    # Secure storage option\n    else:\n        try:\n            logger.info(f\"Loading model {self.model_name} from secure storage (version: {version or 'latest'})\")\n\n            # Create a temporary file to store the model\n            with tempfile.NamedTemporaryFile(suffix=\".pkl\", delete=False) as temp_file:\n                temp_path = temp_file.name\n\n            # Load weights to temporary file\n            weights_metadata = secure_weights_manager.load_weights_to_file(\n                model_name=self.model_name, output_path=temp_path, version=version, decrypt=True\n            )\n\n            # Load model from temporary file\n            with open(temp_path, \"rb\") as f:\n                data = pickle.load(f)\n                self.model = data[\"model\"]\n                self.metadata = data[\"metadata\"]\n\n            # Update metadata with any additional info from secure storage\n            if \"params\" in weights_metadata:\n                self.metadata[\"params\"].update(weights_metadata[\"params\"])\n\n            if \"metrics\" in weights_metadata:\n                self.metadata[\"metrics\"].update(weights_metadata[\"metrics\"])\n\n            # Delete the temporary file\n            try:\n                os.unlink(temp_path)\n            except OSError:\n                pass\n\n            logger.info(\n                f\"Successfully loaded model {self.model_name} version {weights_metadata.get('version', 'unknown')}\"\n            )\n\n        except Exception as e:\n            logger.error(f\"Error loading model {self.model_name} from secure storage: {e}\")\n            raise\n</code></pre>"},{"location":"api/models/#src.models.model_base.ModelBase.log_metric","title":"<code>log_metric(name, value)</code>","text":"<p>Log a metric.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Metric name.</p> required <code>value</code> <code>float</code> <p>Metric value.</p> required Source code in <code>src/models/model_base.py</code> <pre><code>def log_metric(self, name: str, value: float) -&gt; None:\n    \"\"\"Log a metric.\n\n    Args:\n        name: Metric name.\n        value: Metric value.\n    \"\"\"\n    self.metadata[\"metrics\"][name] = value\n    logger.info(f\"Metric {name}: {value}\")\n</code></pre>"},{"location":"api/models/#src.models.model_base.ModelBase.predict","title":"<code>predict(X)</code>","text":"<p>Make predictions using the model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[ndarray, DataFrame]</code> <p>Features to predict.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>Predicted values.</p> Source code in <code>src/models/model_base.py</code> <pre><code>def predict(self, X: Union[np.ndarray, pd.DataFrame]) -&gt; np.ndarray:\n    \"\"\"Make predictions using the model.\n\n    Args:\n        X: Features to predict.\n\n    Returns:\n        Predicted values.\n    \"\"\"\n    if self.model is None:\n        raise ValueError(\"Model not trained or loaded. Call train() or load() first.\")\n    raise NotImplementedError(\"Subclasses must implement predict()\")\n</code></pre>"},{"location":"api/models/#src.models.model_base.ModelBase.save","title":"<code>save(filename=None, secure=False, secure_weights_manager=None, version=None)</code>","text":"<p>Save model to disk.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>Optional[str]</code> <p>Filename to save model. If None, use model_name.</p> <code>None</code> <code>secure</code> <code>bool</code> <p>Whether to save with secure weights storage.</p> <code>False</code> <code>secure_weights_manager</code> <p>SecureModelWeights instance for secure storage.</p> <code>None</code> <code>version</code> <code>Optional[str]</code> <p>Version of the model weights. If None, a timestamp-based version is used.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Path to saved model or a dictionary with secure storage details.</p> Source code in <code>src/models/model_base.py</code> <pre><code>def save(\n    self,\n    filename: Optional[str] = None,\n    secure: bool = False,\n    secure_weights_manager=None,\n    version: Optional[str] = None,\n) -&gt; str:\n    \"\"\"Save model to disk.\n\n    Args:\n        filename: Filename to save model. If None, use model_name.\n        secure: Whether to save with secure weights storage.\n        secure_weights_manager: SecureModelWeights instance for secure storage.\n        version: Version of the model weights. If None, a timestamp-based version is used.\n\n    Returns:\n        Path to saved model or a dictionary with secure storage details.\n    \"\"\"\n    if self.model is None:\n        raise ValueError(\"No model to save. Train a model first.\")\n\n    # If secure storage is requested but no manager is provided\n    if secure and secure_weights_manager is None:\n        logger.warning(\n            \"Secure storage requested but no secure_weights_manager provided. Falling back to standard storage.\"\n        )\n        secure = False\n\n    # Standard storage option\n    if not secure:\n        if filename is None:\n            filename = f\"{self.model_name}.pkl\"\n\n        file_path = self.models_dir / filename\n        logger.info(f\"Saving model to {file_path}\")\n\n        try:\n            with open(file_path, \"wb\") as f:\n                pickle.dump({\"model\": self.model, \"metadata\": self.metadata}, f)\n            return str(file_path)\n        except Exception as e:\n            logger.error(f\"Error saving model to {file_path}: {e}\")\n            raise\n\n    # Secure storage option\n    else:\n        try:\n            logger.info(f\"Saving model {self.model_name} with secure storage\")\n\n            # Create a temporary file for the model\n            with tempfile.NamedTemporaryFile(suffix=\".pkl\", delete=False) as temp_file:\n                temp_path = temp_file.name\n\n                # Save model to temporary file\n                pickle.dump({\"model\": self.model, \"metadata\": self.metadata}, temp_file)\n\n            # Store in secure weights manager\n            additional_metadata = {\n                \"model_type\": self.__class__.__name__,\n                \"params\": self.metadata.get(\"params\", {}),\n                \"metrics\": self.metadata.get(\"metrics\", {}),\n            }\n\n            result = secure_weights_manager.store_weights(\n                model_name=self.model_name,\n                weights_file=temp_path,\n                version=version,\n                metadata=additional_metadata,\n                encrypt=True,\n            )\n\n            # Delete the temporary file\n            try:\n                os.unlink(temp_path)\n            except OSError:\n                pass\n\n            # Return information about the stored model\n            return result\n\n        except Exception as e:\n            logger.error(f\"Error saving model {self.model_name} to secure storage: {e}\")\n            raise\n</code></pre>"},{"location":"api/models/#src.models.model_base.ModelBase.set_params","title":"<code>set_params(params)</code>","text":"<p>Set model parameters.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>Dict[str, Any]</code> <p>Dictionary containing model parameters.</p> required Source code in <code>src/models/model_base.py</code> <pre><code>def set_params(self, params: Dict[str, Any]) -&gt; None:\n    \"\"\"Set model parameters.\n\n    Args:\n        params: Dictionary containing model parameters.\n    \"\"\"\n    self.metadata[\"params\"].update(params)\n</code></pre>"},{"location":"api/models/#src.models.model_base.ModelBase.train","title":"<code>train(X, y, **kwargs)</code>","text":"<p>Train the model.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[ndarray, DataFrame]</code> <p>Training features.</p> required <code>y</code> <code>Union[ndarray, Series]</code> <p>Training labels.</p> required <code>**kwargs</code> <p>Additional training parameters.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing training metrics.</p> Source code in <code>src/models/model_base.py</code> <pre><code>def train(self, X: Union[np.ndarray, pd.DataFrame], y: Union[np.ndarray, pd.Series], **kwargs) -&gt; Dict[str, Any]:\n    \"\"\"Train the model.\n\n    Args:\n        X: Training features.\n        y: Training labels.\n        **kwargs: Additional training parameters.\n\n    Returns:\n        Dictionary containing training metrics.\n    \"\"\"\n    raise NotImplementedError(\"Subclasses must implement train()\")\n</code></pre>"},{"location":"api/models/#securemodelweights","title":"SecureModelWeights","text":""},{"location":"api/models/#src.models.secure_weights.SecureModelWeights","title":"<code>src.models.secure_weights.SecureModelWeights</code>","text":"<p>Secure storage and management of model weights.</p> Source code in <code>src/models/secure_weights.py</code> <pre><code>class SecureModelWeights:\n    \"\"\"Secure storage and management of model weights.\"\"\"\n\n    def __init__(\n        self,\n        encryption_service: EncryptionService,\n        secret_manager: Optional[SecretManager] = None,\n        config: Optional[Dict[str, Any]] = None,\n    ):\n        \"\"\"Initialize secure model weights manager.\n\n        Args:\n            encryption_service: Encryption service for encrypting/decrypting model weights.\n            secret_manager: Secret manager for managing access credentials.\n            config: Configuration for secure model weights.\n        \"\"\"\n        self.encryption_service = encryption_service\n        self.secret_manager = secret_manager\n        self.config = config or {}\n\n        # Storage configuration\n        self.storage_config = self.config.get(\"storage\", {})\n        self.primary_storage = self.storage_config.get(\"primary\", \"s3\")\n        self.replicate_to = self.storage_config.get(\"replicate_to\", [])\n        self.versioning_enabled = self.storage_config.get(\"versioning_enabled\", True)\n        self.checksum_algorithm = self.storage_config.get(\"checksum_algorithm\", \"sha256\")\n\n        # Cache configuration\n        self.cache_config = self.config.get(\"cache\", {})\n        self.cache_enabled = self.cache_config.get(\"enabled\", True)\n        self.cache_dir = Path(self.cache_config.get(\"directory\", \"weights_cache\"))\n        self.cache_max_size_gb = self.cache_config.get(\"max_size_gb\", 10)\n\n        # Create cache directory if it doesn't exist\n        if self.cache_enabled:\n            os.makedirs(self.cache_dir, exist_ok=True)\n\n        # Access control\n        self.access_control_enabled = self.config.get(\"access_control_enabled\", True)\n\n        # Metadata storage\n        self.metadata = {}\n        self.metadata_lock = threading.Lock()\n\n        logger.info(\n            f\"Initialized SecureModelWeights with primary storage: {self.primary_storage}, \"\n            f\"replication: {self.replicate_to}, versioning: {self.versioning_enabled}\"\n        )\n\n    def store_weights(\n        self,\n        model_name: str,\n        weights_file: Union[str, Path, BinaryIO],\n        version: Optional[str] = None,\n        metadata: Optional[Dict[str, Any]] = None,\n        encrypt: bool = True,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Store model weights securely.\n\n        Args:\n            model_name: Name of the model.\n            weights_file: Path to weights file or file-like object.\n            version: Version of the weights. If None, a timestamp-based version is used.\n            metadata: Additional metadata for the weights.\n            encrypt: Whether to encrypt the weights.\n\n        Returns:\n            Dictionary containing details of the stored weights.\n        \"\"\"\n        # Generate version if not provided\n        if version is None:\n            version = f\"v_{int(time.time())}\"\n\n        # Normalize model name\n        model_name = model_name.replace(\" \", \"_\").lower()\n\n        # Get file data and calculate checksum\n        if isinstance(weights_file, (str, Path)):\n            weights_path = Path(weights_file)\n            if not weights_path.exists():\n                raise FileNotFoundError(f\"Weights file {weights_path} not found\")\n\n            with open(weights_path, \"rb\") as f:\n                weights_data = f.read()\n        else:\n            # File-like object\n            weights_data = weights_file.read()\n            if hasattr(weights_file, \"seek\"):\n                weights_file.seek(0)\n\n        # Calculate checksum\n        checksum = self._calculate_checksum(weights_data)\n\n        # Prepare metadata\n        weights_metadata = {\n            \"model_name\": model_name,\n            \"version\": version,\n            \"timestamp\": int(time.time()),\n            \"checksum\": checksum,\n            \"checksum_algorithm\": self.checksum_algorithm,\n            \"size_bytes\": len(weights_data),\n            \"encrypted\": encrypt,\n            \"storage_locations\": [],\n        }\n\n        # Add any additional metadata\n        if metadata:\n            weights_metadata.update(metadata)\n\n        # Encrypt weights if required\n        if encrypt:\n            weights_data = self.encryption_service.encrypt_data(weights_data)\n            weights_metadata[\"encrypted\"] = True\n\n        # Store in primary storage\n        primary_location = self._store_to_provider(\n            self.primary_storage, model_name, version, weights_data, weights_metadata\n        )\n        weights_metadata[\"storage_locations\"].append(primary_location)\n\n        # Replicate to other providers if configured\n        for provider in self.replicate_to:\n            try:\n                location = self._store_to_provider(provider, model_name, version, weights_data, weights_metadata)\n                weights_metadata[\"storage_locations\"].append(location)\n            except Exception as e:\n                logger.error(f\"Error replicating weights to {provider}: {e}\")\n\n        # Store in local cache if enabled\n        if self.cache_enabled:\n            self._store_in_cache(model_name, version, weights_data, weights_metadata)\n\n        # Update metadata registry\n        with self.metadata_lock:\n            if model_name not in self.metadata:\n                self.metadata[model_name] = {}\n            self.metadata[model_name][version] = weights_metadata\n\n        logger.info(\n            f\"Stored weights for {model_name} version {version}, \"\n            f\"size: {len(weights_data)/1024/1024:.2f} MB, \"\n            f\"checksum: {checksum[:8]}..., encrypted: {encrypt}\"\n        )\n\n        return weights_metadata\n\n    def load_weights(\n        self, model_name: str, version: Optional[str] = None, decrypt: bool = True, provider: Optional[str] = None\n    ) -&gt; Tuple[bytes, Dict[str, Any]]:\n        \"\"\"Load model weights securely.\n\n        Args:\n            model_name: Name of the model.\n            version: Version of the weights. If None, the latest version is used.\n            decrypt: Whether to decrypt the weights.\n            provider: Storage provider to load from. If None, try all available locations.\n\n        Returns:\n            Tuple of (weights_data, metadata).\n        \"\"\"\n        # Normalize model name\n        model_name = model_name.replace(\" \", \"_\").lower()\n\n        # Get version if not specified\n        if version is None:\n            version = self._get_latest_version(model_name)\n            if version is None:\n                raise ValueError(f\"No versions found for model {model_name}\")\n\n        # Check if we have metadata for this model and version\n        weights_metadata = self._get_weights_metadata(model_name, version)\n        if not weights_metadata:\n            raise ValueError(f\"No metadata found for model {model_name} version {version}\")\n\n        # Check if weights are in cache\n        if self.cache_enabled and not provider:\n            cache_data = self._load_from_cache(model_name, version)\n            if cache_data is not None:\n                logger.info(f\"Loaded weights for {model_name} version {version} from cache\")\n                weights_data = cache_data\n\n                # Verify checksum\n                calculated_checksum = self._calculate_checksum(\n                    weights_data\n                    if not decrypt or not weights_metadata.get(\"encrypted\", False)\n                    else self.encryption_service.decrypt_data(weights_data)\n                )\n\n                if calculated_checksum != weights_metadata[\"checksum\"]:\n                    logger.warning(\n                        f\"Checksum mismatch for cached weights of {model_name} version {version}. \"\n                        f\"Expected {weights_metadata['checksum']}, got {calculated_checksum}. \"\n                        f\"Loading from storage instead.\"\n                    )\n                else:\n                    # Decrypt if required\n                    if decrypt and weights_metadata.get(\"encrypted\", False):\n                        weights_data = self.encryption_service.decrypt_data(weights_data)\n\n                    return weights_data, weights_metadata\n\n        # Load from specified provider or try all available locations\n        if provider:\n            providers_to_try = [provider]\n        else:\n            # Get providers from metadata\n            providers_to_try = [location[\"provider\"] for location in weights_metadata.get(\"storage_locations\", [])]\n\n        # Try to load from each provider\n        last_error = None\n        for provider_name in providers_to_try:\n            try:\n                weights_data = self._load_from_provider(provider_name, model_name, version, weights_metadata)\n\n                # Verify checksum\n                calculated_checksum = self._calculate_checksum(\n                    weights_data\n                    if not decrypt or not weights_metadata.get(\"encrypted\", False)\n                    else self.encryption_service.decrypt_data(weights_data)\n                )\n\n                if calculated_checksum != weights_metadata[\"checksum\"]:\n                    logger.warning(\n                        f\"Checksum mismatch for weights of {model_name} version {version} \"\n                        f\"from provider {provider_name}. Expected {weights_metadata['checksum']}, \"\n                        f\"got {calculated_checksum}. Trying another provider.\"\n                    )\n                    continue\n\n                # Store in cache if enabled\n                if self.cache_enabled:\n                    self._store_in_cache(model_name, version, weights_data, weights_metadata)\n\n                # Decrypt if required\n                if decrypt and weights_metadata.get(\"encrypted\", False):\n                    weights_data = self.encryption_service.decrypt_data(weights_data)\n\n                logger.info(f\"Loaded weights for {model_name} version {version} \" f\"from provider {provider_name}\")\n\n                return weights_data, weights_metadata\n\n            except Exception as e:\n                logger.error(\n                    f\"Error loading weights for {model_name} version {version} \" f\"from provider {provider_name}: {e}\"\n                )\n                last_error = e\n\n        # If we get here, we failed to load from any provider\n        raise RuntimeError(\n            f\"Failed to load weights for {model_name} version {version} \" f\"from any provider: {last_error}\"\n        )\n\n    def load_weights_to_file(\n        self,\n        model_name: str,\n        output_path: Union[str, Path],\n        version: Optional[str] = None,\n        decrypt: bool = True,\n        provider: Optional[str] = None,\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Load model weights to a file.\n\n        Args:\n            model_name: Name of the model.\n            output_path: Path to save the weights.\n            version: Version of the weights. If None, the latest version is used.\n            decrypt: Whether to decrypt the weights.\n            provider: Storage provider to load from. If None, try all available locations.\n\n        Returns:\n            Metadata for the loaded weights.\n        \"\"\"\n        # Load weights\n        weights_data, weights_metadata = self.load_weights(model_name, version, decrypt, provider)\n\n        # Save to file\n        output_path = Path(output_path)\n        with open(output_path, \"wb\") as f:\n            f.write(weights_data)\n\n        logger.info(\n            f\"Saved weights for {model_name} version {version} to {output_path}, \"\n            f\"size: {len(weights_data)/1024/1024:.2f} MB\"\n        )\n\n        return weights_metadata\n\n    def delete_weights(self, model_name: str, version: Optional[str] = None, provider: Optional[str] = None) -&gt; bool:\n        \"\"\"Delete model weights.\n\n        Args:\n            model_name: Name of the model.\n            version: Version of the weights. If None, delete all versions.\n            provider: Storage provider to delete from. If None, delete from all providers.\n\n        Returns:\n            True if successful, False otherwise.\n        \"\"\"\n        # Normalize model name\n        model_name = model_name.replace(\" \", \"_\").lower()\n\n        # Get versions to delete\n        versions_to_delete = []\n        if version is None:\n            # Delete all versions\n            with self.metadata_lock:\n                if model_name in self.metadata:\n                    versions_to_delete = list(self.metadata[model_name].keys())\n        else:\n            versions_to_delete = [version]\n\n        if not versions_to_delete:\n            logger.warning(f\"No versions found for model {model_name}\")\n            return False\n\n        # Delete each version\n        success = True\n        for ver in versions_to_delete:\n            # Get metadata\n            weights_metadata = self._get_weights_metadata(model_name, ver)\n            if not weights_metadata:\n                logger.warning(f\"No metadata found for model {model_name} version {ver}\")\n                success = False\n                continue\n\n            # Get providers to delete from\n            if provider:\n                providers_to_delete = [provider]\n            else:\n                # Get providers from metadata\n                providers_to_delete = [\n                    location[\"provider\"] for location in weights_metadata.get(\"storage_locations\", [])\n                ]\n\n            # Delete from each provider\n            for provider_name in providers_to_delete:\n                try:\n                    self._delete_from_provider(provider_name, model_name, ver, weights_metadata)\n                    logger.info(f\"Deleted weights for {model_name} version {ver} \" f\"from provider {provider_name}\")\n                except Exception as e:\n                    logger.error(\n                        f\"Error deleting weights for {model_name} version {ver} \" f\"from provider {provider_name}: {e}\"\n                    )\n                    success = False\n\n            # Delete from cache\n            if self.cache_enabled:\n                self._delete_from_cache(model_name, ver)\n\n            # Update metadata\n            with self.metadata_lock:\n                if model_name in self.metadata and ver in self.metadata[model_name]:\n                    del self.metadata[model_name][ver]\n\n                    # Remove model if no versions left\n                    if not self.metadata[model_name]:\n                        del self.metadata[model_name]\n\n        return success\n\n    def list_models(self) -&gt; List[str]:\n        \"\"\"List all models with stored weights.\n\n        Returns:\n            List of model names.\n        \"\"\"\n        with self.metadata_lock:\n            return list(self.metadata.keys())\n\n    def list_versions(self, model_name: str) -&gt; List[Dict[str, Any]]:\n        \"\"\"List all versions for a model.\n\n        Args:\n            model_name: Name of the model.\n\n        Returns:\n            List of version metadata.\n        \"\"\"\n        # Normalize model name\n        model_name = model_name.replace(\" \", \"_\").lower()\n\n        with self.metadata_lock:\n            if model_name not in self.metadata:\n                return []\n\n            return [\n                {\n                    \"version\": version,\n                    \"timestamp\": metadata.get(\"timestamp\"),\n                    \"size_bytes\": metadata.get(\"size_bytes\"),\n                    \"storage_locations\": [loc.get(\"provider\") for loc in metadata.get(\"storage_locations\", [])],\n                }\n                for version, metadata in self.metadata[model_name].items()\n            ]\n\n    def get_weights_info(self, model_name: str, version: Optional[str] = None) -&gt; Dict[str, Any]:\n        \"\"\"Get information about model weights.\n\n        Args:\n            model_name: Name of the model.\n            version: Version of the weights. If None, get info for all versions.\n\n        Returns:\n            Dictionary containing weights information.\n        \"\"\"\n        # Normalize model name\n        model_name = model_name.replace(\" \", \"_\").lower()\n\n        if version is None:\n            # Get info for all versions\n            versions = self.list_versions(model_name)\n            return {\"model_name\": model_name, \"versions\": versions, \"version_count\": len(versions)}\n        else:\n            # Get info for specific version\n            weights_metadata = self._get_weights_metadata(model_name, version)\n            if not weights_metadata:\n                return {}\n\n            return weights_metadata\n\n    def verify_weights_integrity(\n        self, model_name: str, version: Optional[str] = None, provider: Optional[str] = None\n    ) -&gt; Dict[str, bool]:\n        \"\"\"Verify the integrity of model weights.\n\n        Args:\n            model_name: Name of the model.\n            version: Version of the weights. If None, verify all versions.\n            provider: Storage provider to verify. If None, verify all providers.\n\n        Returns:\n            Dictionary mapping provider to verification result.\n        \"\"\"\n        # Normalize model name\n        model_name = model_name.replace(\" \", \"_\").lower()\n\n        # Get versions to verify\n        versions_to_verify = []\n        if version is None:\n            # Verify all versions\n            with self.metadata_lock:\n                if model_name in self.metadata:\n                    versions_to_verify = list(self.metadata[model_name].keys())\n        else:\n            versions_to_verify = [version]\n\n        if not versions_to_verify:\n            logger.warning(f\"No versions found for model {model_name}\")\n            return {}\n\n        # Verify each version\n        results = {}\n        for ver in versions_to_verify:\n            # Get metadata\n            weights_metadata = self._get_weights_metadata(model_name, ver)\n            if not weights_metadata:\n                logger.warning(f\"No metadata found for model {model_name} version {ver}\")\n                continue\n\n            # Get providers to verify\n            if provider:\n                providers_to_verify = [provider]\n            else:\n                # Get providers from metadata\n                providers_to_verify = [\n                    location[\"provider\"] for location in weights_metadata.get(\"storage_locations\", [])\n                ]\n\n            # Verify each provider\n            for provider_name in providers_to_verify:\n                try:\n                    # Load weights\n                    weights_data = self._load_from_provider(provider_name, model_name, ver, weights_metadata)\n\n                    # Calculate checksum\n                    if weights_metadata.get(\"encrypted\", False):\n                        # Decrypt first to get the original checksum\n                        decrypted_data = self.encryption_service.decrypt_data(weights_data)\n                        calculated_checksum = self._calculate_checksum(decrypted_data)\n                    else:\n                        calculated_checksum = self._calculate_checksum(weights_data)\n\n                    # Verify checksum\n                    expected_checksum = weights_metadata.get(\"checksum\")\n                    if calculated_checksum == expected_checksum:\n                        results[f\"{model_name}/{ver}/{provider_name}\"] = True\n                        logger.info(\n                            f\"Verified integrity of weights for {model_name} version {ver} \"\n                            f\"from provider {provider_name}: OK\"\n                        )\n                    else:\n                        results[f\"{model_name}/{ver}/{provider_name}\"] = False\n                        logger.warning(\n                            f\"Integrity check failed for weights of {model_name} version {ver} \"\n                            f\"from provider {provider_name}. Expected {expected_checksum}, \"\n                            f\"got {calculated_checksum}.\"\n                        )\n                except Exception as e:\n                    results[f\"{model_name}/{ver}/{provider_name}\"] = False\n                    logger.error(\n                        f\"Error verifying weights for {model_name} version {ver} \" f\"from provider {provider_name}: {e}\"\n                    )\n\n        return results\n\n    def _calculate_checksum(self, data: bytes) -&gt; str:\n        \"\"\"Calculate checksum for data.\n\n        Args:\n            data: Data to calculate checksum for.\n\n        Returns:\n            Checksum as hex string.\n        \"\"\"\n        if self.checksum_algorithm == \"sha256\":\n            return hashlib.sha256(data).hexdigest()\n        elif self.checksum_algorithm == \"md5\":\n            return hashlib.md5(data).hexdigest()\n        else:\n            raise ValueError(f\"Unsupported checksum algorithm: {self.checksum_algorithm}\")\n\n    def _store_to_provider(\n        self, provider: str, model_name: str, version: str, weights_data: bytes, metadata: Dict[str, Any]\n    ) -&gt; Dict[str, Any]:\n        \"\"\"Store weights to a specific storage provider.\n\n        Args:\n            provider: Storage provider name.\n            model_name: Name of the model.\n            version: Version of the weights.\n            weights_data: Model weights data.\n            metadata: Weights metadata.\n\n        Returns:\n            Storage location metadata.\n        \"\"\"\n        # This would be implemented to store weights in different cloud providers\n        # For now, we'll implement a simplified local filesystem storage\n\n        if provider == \"local\":\n            storage_dir = Path(self.storage_config.get(\"local_path\", \"secure_weights\"))\n            os.makedirs(storage_dir, exist_ok=True)\n\n            # Create model directory\n            model_dir = storage_dir / model_name\n            os.makedirs(model_dir, exist_ok=True)\n\n            # Save weights file\n            weights_path = model_dir / f\"{version}.weights\"\n            with open(weights_path, \"wb\") as f:\n                f.write(weights_data)\n\n            # Save metadata\n            metadata_path = model_dir / f\"{version}.metadata.json\"\n            with open(metadata_path, \"w\") as f:\n                json.dump(metadata, f)\n\n            # Return location info\n            return {\"provider\": \"local\", \"path\": str(weights_path), \"metadata_path\": str(metadata_path)}\n\n        elif provider == \"s3\":\n            # This would use boto3 to store in S3\n            # For simplicity, we'll just log and return a mock response\n            logger.info(\n                f\"Would store weights for {model_name} version {version} in S3 \"\n                f\"bucket {self.storage_config.get('s3_bucket', 'llm-models')}\"\n            )\n\n            return {\n                \"provider\": \"s3\",\n                \"bucket\": self.storage_config.get(\"s3_bucket\", \"llm-models\"),\n                \"key\": f\"{model_name}/{version}.weights\",\n            }\n\n        elif provider == \"gcs\":\n            # This would use Google Cloud Storage client to store in GCS\n            # For simplicity, we'll just log and return a mock response\n            logger.info(\n                f\"Would store weights for {model_name} version {version} in GCS \"\n                f\"bucket {self.storage_config.get('gcs_bucket', 'llm-models')}\"\n            )\n\n            return {\n                \"provider\": \"gcs\",\n                \"bucket\": self.storage_config.get(\"gcs_bucket\", \"llm-models\"),\n                \"path\": f\"{model_name}/{version}.weights\",\n            }\n\n        elif provider == \"azure\":\n            # This would use Azure Blob Storage client\n            # For simplicity, we'll just log and return a mock response\n            logger.info(\n                f\"Would store weights for {model_name} version {version} in Azure \"\n                f\"container {self.storage_config.get('azure_container', 'llm-models')}\"\n            )\n\n            return {\n                \"provider\": \"azure\",\n                \"container\": self.storage_config.get(\"azure_container\", \"llm-models\"),\n                \"blob\": f\"{model_name}/{version}.weights\",\n            }\n\n        else:\n            raise ValueError(f\"Unsupported storage provider: {provider}\")\n\n    def _load_from_provider(self, provider: str, model_name: str, version: str, metadata: Dict[str, Any]) -&gt; bytes:\n        \"\"\"Load weights from a specific storage provider.\n\n        Args:\n            provider: Storage provider name.\n            model_name: Name of the model.\n            version: Version of the weights.\n            metadata: Weights metadata.\n\n        Returns:\n            Model weights data.\n        \"\"\"\n        # This would be implemented to load weights from different cloud providers\n        # For now, we'll implement a simplified local filesystem storage\n\n        if provider == \"local\":\n            # Find location info\n            location = next(\n                (loc for loc in metadata.get(\"storage_locations\", []) if loc.get(\"provider\") == \"local\"), None\n            )\n\n            if not location:\n                raise ValueError(f\"No local storage location found for model {model_name} version {version}\")\n\n            # Load weights\n            weights_path = location.get(\"path\")\n            if not weights_path or not os.path.exists(weights_path):\n                raise FileNotFoundError(\n                    f\"Weights file {weights_path} not found for model {model_name} version {version}\"\n                )\n\n            with open(weights_path, \"rb\") as f:\n                return f.read()\n\n        # For other providers, we'd implement the actual loading logic\n        # For now, we'll raise an error\n        raise NotImplementedError(f\"Loading from provider {provider} not implemented\")\n\n    def _delete_from_provider(self, provider: str, model_name: str, version: str, metadata: Dict[str, Any]) -&gt; bool:\n        \"\"\"Delete weights from a specific storage provider.\n\n        Args:\n            provider: Storage provider name.\n            model_name: Name of the model.\n            version: Version of the weights.\n            metadata: Weights metadata.\n\n        Returns:\n            True if successful, False otherwise.\n        \"\"\"\n        # This would be implemented to delete weights from different cloud providers\n        # For now, we'll implement a simplified local filesystem storage\n\n        if provider == \"local\":\n            # Find location info\n            location = next(\n                (loc for loc in metadata.get(\"storage_locations\", []) if loc.get(\"provider\") == \"local\"), None\n            )\n\n            if not location:\n                raise ValueError(f\"No local storage location found for model {model_name} version {version}\")\n\n            # Delete weights\n            weights_path = location.get(\"path\")\n            if weights_path and os.path.exists(weights_path):\n                os.remove(weights_path)\n\n            # Delete metadata\n            metadata_path = location.get(\"metadata_path\")\n            if metadata_path and os.path.exists(metadata_path):\n                os.remove(metadata_path)\n\n            return True\n\n        # For other providers, we'd implement the actual deletion logic\n        # For now, we'll just log and return success\n        logger.info(f\"Would delete weights for {model_name} version {version} from provider {provider}\")\n\n        return True\n\n    def _store_in_cache(self, model_name: str, version: str, weights_data: bytes, metadata: Dict[str, Any]) -&gt; None:\n        \"\"\"Store weights in local cache.\n\n        Args:\n            model_name: Name of the model.\n            version: Version of the weights.\n            weights_data: Model weights data.\n            metadata: Weights metadata.\n        \"\"\"\n        # Create model directory in cache\n        model_cache_dir = self.cache_dir / model_name\n        os.makedirs(model_cache_dir, exist_ok=True)\n\n        # Save weights file\n        weights_path = model_cache_dir / f\"{version}.weights\"\n        with open(weights_path, \"wb\") as f:\n            f.write(weights_data)\n\n        # Save metadata\n        metadata_path = model_cache_dir / f\"{version}.metadata.json\"\n        with open(metadata_path, \"w\") as f:\n            json.dump(metadata, f)\n\n        # Check cache size and cleanup if necessary\n        self._cleanup_cache()\n\n    def _load_from_cache(self, model_name: str, version: str) -&gt; Optional[bytes]:\n        \"\"\"Load weights from local cache.\n\n        Args:\n            model_name: Name of the model.\n            version: Version of the weights.\n\n        Returns:\n            Model weights data or None if not in cache.\n        \"\"\"\n        weights_path = self.cache_dir / model_name / f\"{version}.weights\"\n        if not weights_path.exists():\n            return None\n\n        with open(weights_path, \"rb\") as f:\n            return f.read()\n\n    def _delete_from_cache(self, model_name: str, version: str) -&gt; None:\n        \"\"\"Delete weights from local cache.\n\n        Args:\n            model_name: Name of the model.\n            version: Version of the weights.\n        \"\"\"\n        weights_path = self.cache_dir / model_name / f\"{version}.weights\"\n        if weights_path.exists():\n            os.remove(weights_path)\n\n        metadata_path = self.cache_dir / model_name / f\"{version}.metadata.json\"\n        if metadata_path.exists():\n            os.remove(metadata_path)\n\n    def _cleanup_cache(self) -&gt; None:\n        \"\"\"Clean up cache if it exceeds the maximum size.\"\"\"\n        # Get cache size\n        cache_size = sum(f.stat().st_size for f in self.cache_dir.glob(\"**/*\") if f.is_file())\n        cache_size_gb = cache_size / 1024 / 1024 / 1024\n\n        # Check if cache size exceeds maximum\n        if cache_size_gb &lt;= self.cache_max_size_gb:\n            return\n\n        logger.info(\n            f\"Cache size {cache_size_gb:.2f} GB exceeds maximum {self.cache_max_size_gb} GB, \" f\"cleaning up...\"\n        )\n\n        # Get all weights files with their last access time\n        weight_files = []\n        for weights_path in self.cache_dir.glob(\"**/*.weights\"):\n            try:\n                # Use last access time as a proxy for LRU\n                atime = os.path.getatime(weights_path)\n                weight_files.append((weights_path, atime))\n            except Exception:\n                continue\n\n        # Sort by access time (oldest first)\n        weight_files.sort(key=lambda x: x[1])\n\n        # Delete files until cache size is below maximum\n        for weights_path, _ in weight_files:\n            if cache_size_gb &lt;= self.cache_max_size_gb * 0.8:  # Leave some buffer\n                break\n\n            try:\n                # Get file size\n                file_size = weights_path.stat().st_size\n\n                # Delete weights file\n                os.remove(weights_path)\n\n                # Delete metadata file\n                metadata_path = weights_path.with_name(weights_path.name.replace(\".weights\", \".metadata.json\"))\n                if metadata_path.exists():\n                    os.remove(metadata_path)\n\n                # Update cache size\n                cache_size -= file_size\n                cache_size_gb = cache_size / 1024 / 1024 / 1024\n\n                logger.info(f\"Removed {weights_path} from cache\")\n            except Exception as e:\n                logger.error(f\"Error removing {weights_path} from cache: {e}\")\n\n    def _get_weights_metadata(self, model_name: str, version: str) -&gt; Optional[Dict[str, Any]]:\n        \"\"\"Get metadata for model weights.\n\n        Args:\n            model_name: Name of the model.\n            version: Version of the weights.\n\n        Returns:\n            Weights metadata or None if not found.\n        \"\"\"\n        with self.metadata_lock:\n            if model_name not in self.metadata:\n                return None\n\n            return self.metadata[model_name].get(version)\n\n    def _get_latest_version(self, model_name: str) -&gt; Optional[str]:\n        \"\"\"Get the latest version for a model.\n\n        Args:\n            model_name: Name of the model.\n\n        Returns:\n            Latest version or None if no versions found.\n        \"\"\"\n        with self.metadata_lock:\n            if model_name not in self.metadata:\n                return None\n\n            # Get all versions with timestamps\n            versions = [\n                (version, metadata.get(\"timestamp\", 0)) for version, metadata in self.metadata[model_name].items()\n            ]\n\n            # Sort by timestamp (newest first)\n            versions.sort(key=lambda x: x[1], reverse=True)\n\n            # Return latest version\n            return versions[0][0] if versions else None\n</code></pre>"},{"location":"api/models/#src.models.secure_weights.SecureModelWeights.__init__","title":"<code>__init__(encryption_service, secret_manager=None, config=None)</code>","text":"<p>Initialize secure model weights manager.</p> <p>Parameters:</p> Name Type Description Default <code>encryption_service</code> <code>EncryptionService</code> <p>Encryption service for encrypting/decrypting model weights.</p> required <code>secret_manager</code> <code>Optional[SecretManager]</code> <p>Secret manager for managing access credentials.</p> <code>None</code> <code>config</code> <code>Optional[Dict[str, Any]]</code> <p>Configuration for secure model weights.</p> <code>None</code> Source code in <code>src/models/secure_weights.py</code> <pre><code>def __init__(\n    self,\n    encryption_service: EncryptionService,\n    secret_manager: Optional[SecretManager] = None,\n    config: Optional[Dict[str, Any]] = None,\n):\n    \"\"\"Initialize secure model weights manager.\n\n    Args:\n        encryption_service: Encryption service for encrypting/decrypting model weights.\n        secret_manager: Secret manager for managing access credentials.\n        config: Configuration for secure model weights.\n    \"\"\"\n    self.encryption_service = encryption_service\n    self.secret_manager = secret_manager\n    self.config = config or {}\n\n    # Storage configuration\n    self.storage_config = self.config.get(\"storage\", {})\n    self.primary_storage = self.storage_config.get(\"primary\", \"s3\")\n    self.replicate_to = self.storage_config.get(\"replicate_to\", [])\n    self.versioning_enabled = self.storage_config.get(\"versioning_enabled\", True)\n    self.checksum_algorithm = self.storage_config.get(\"checksum_algorithm\", \"sha256\")\n\n    # Cache configuration\n    self.cache_config = self.config.get(\"cache\", {})\n    self.cache_enabled = self.cache_config.get(\"enabled\", True)\n    self.cache_dir = Path(self.cache_config.get(\"directory\", \"weights_cache\"))\n    self.cache_max_size_gb = self.cache_config.get(\"max_size_gb\", 10)\n\n    # Create cache directory if it doesn't exist\n    if self.cache_enabled:\n        os.makedirs(self.cache_dir, exist_ok=True)\n\n    # Access control\n    self.access_control_enabled = self.config.get(\"access_control_enabled\", True)\n\n    # Metadata storage\n    self.metadata = {}\n    self.metadata_lock = threading.Lock()\n\n    logger.info(\n        f\"Initialized SecureModelWeights with primary storage: {self.primary_storage}, \"\n        f\"replication: {self.replicate_to}, versioning: {self.versioning_enabled}\"\n    )\n</code></pre>"},{"location":"api/models/#src.models.secure_weights.SecureModelWeights.delete_weights","title":"<code>delete_weights(model_name, version=None, provider=None)</code>","text":"<p>Delete model weights.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model.</p> required <code>version</code> <code>Optional[str]</code> <p>Version of the weights. If None, delete all versions.</p> <code>None</code> <code>provider</code> <code>Optional[str]</code> <p>Storage provider to delete from. If None, delete from all providers.</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if successful, False otherwise.</p> Source code in <code>src/models/secure_weights.py</code> <pre><code>def delete_weights(self, model_name: str, version: Optional[str] = None, provider: Optional[str] = None) -&gt; bool:\n    \"\"\"Delete model weights.\n\n    Args:\n        model_name: Name of the model.\n        version: Version of the weights. If None, delete all versions.\n        provider: Storage provider to delete from. If None, delete from all providers.\n\n    Returns:\n        True if successful, False otherwise.\n    \"\"\"\n    # Normalize model name\n    model_name = model_name.replace(\" \", \"_\").lower()\n\n    # Get versions to delete\n    versions_to_delete = []\n    if version is None:\n        # Delete all versions\n        with self.metadata_lock:\n            if model_name in self.metadata:\n                versions_to_delete = list(self.metadata[model_name].keys())\n    else:\n        versions_to_delete = [version]\n\n    if not versions_to_delete:\n        logger.warning(f\"No versions found for model {model_name}\")\n        return False\n\n    # Delete each version\n    success = True\n    for ver in versions_to_delete:\n        # Get metadata\n        weights_metadata = self._get_weights_metadata(model_name, ver)\n        if not weights_metadata:\n            logger.warning(f\"No metadata found for model {model_name} version {ver}\")\n            success = False\n            continue\n\n        # Get providers to delete from\n        if provider:\n            providers_to_delete = [provider]\n        else:\n            # Get providers from metadata\n            providers_to_delete = [\n                location[\"provider\"] for location in weights_metadata.get(\"storage_locations\", [])\n            ]\n\n        # Delete from each provider\n        for provider_name in providers_to_delete:\n            try:\n                self._delete_from_provider(provider_name, model_name, ver, weights_metadata)\n                logger.info(f\"Deleted weights for {model_name} version {ver} \" f\"from provider {provider_name}\")\n            except Exception as e:\n                logger.error(\n                    f\"Error deleting weights for {model_name} version {ver} \" f\"from provider {provider_name}: {e}\"\n                )\n                success = False\n\n        # Delete from cache\n        if self.cache_enabled:\n            self._delete_from_cache(model_name, ver)\n\n        # Update metadata\n        with self.metadata_lock:\n            if model_name in self.metadata and ver in self.metadata[model_name]:\n                del self.metadata[model_name][ver]\n\n                # Remove model if no versions left\n                if not self.metadata[model_name]:\n                    del self.metadata[model_name]\n\n    return success\n</code></pre>"},{"location":"api/models/#src.models.secure_weights.SecureModelWeights.get_weights_info","title":"<code>get_weights_info(model_name, version=None)</code>","text":"<p>Get information about model weights.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model.</p> required <code>version</code> <code>Optional[str]</code> <p>Version of the weights. If None, get info for all versions.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing weights information.</p> Source code in <code>src/models/secure_weights.py</code> <pre><code>def get_weights_info(self, model_name: str, version: Optional[str] = None) -&gt; Dict[str, Any]:\n    \"\"\"Get information about model weights.\n\n    Args:\n        model_name: Name of the model.\n        version: Version of the weights. If None, get info for all versions.\n\n    Returns:\n        Dictionary containing weights information.\n    \"\"\"\n    # Normalize model name\n    model_name = model_name.replace(\" \", \"_\").lower()\n\n    if version is None:\n        # Get info for all versions\n        versions = self.list_versions(model_name)\n        return {\"model_name\": model_name, \"versions\": versions, \"version_count\": len(versions)}\n    else:\n        # Get info for specific version\n        weights_metadata = self._get_weights_metadata(model_name, version)\n        if not weights_metadata:\n            return {}\n\n        return weights_metadata\n</code></pre>"},{"location":"api/models/#src.models.secure_weights.SecureModelWeights.list_models","title":"<code>list_models()</code>","text":"<p>List all models with stored weights.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of model names.</p> Source code in <code>src/models/secure_weights.py</code> <pre><code>def list_models(self) -&gt; List[str]:\n    \"\"\"List all models with stored weights.\n\n    Returns:\n        List of model names.\n    \"\"\"\n    with self.metadata_lock:\n        return list(self.metadata.keys())\n</code></pre>"},{"location":"api/models/#src.models.secure_weights.SecureModelWeights.list_versions","title":"<code>list_versions(model_name)</code>","text":"<p>List all versions for a model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model.</p> required <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>List of version metadata.</p> Source code in <code>src/models/secure_weights.py</code> <pre><code>def list_versions(self, model_name: str) -&gt; List[Dict[str, Any]]:\n    \"\"\"List all versions for a model.\n\n    Args:\n        model_name: Name of the model.\n\n    Returns:\n        List of version metadata.\n    \"\"\"\n    # Normalize model name\n    model_name = model_name.replace(\" \", \"_\").lower()\n\n    with self.metadata_lock:\n        if model_name not in self.metadata:\n            return []\n\n        return [\n            {\n                \"version\": version,\n                \"timestamp\": metadata.get(\"timestamp\"),\n                \"size_bytes\": metadata.get(\"size_bytes\"),\n                \"storage_locations\": [loc.get(\"provider\") for loc in metadata.get(\"storage_locations\", [])],\n            }\n            for version, metadata in self.metadata[model_name].items()\n        ]\n</code></pre>"},{"location":"api/models/#src.models.secure_weights.SecureModelWeights.load_weights","title":"<code>load_weights(model_name, version=None, decrypt=True, provider=None)</code>","text":"<p>Load model weights securely.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model.</p> required <code>version</code> <code>Optional[str]</code> <p>Version of the weights. If None, the latest version is used.</p> <code>None</code> <code>decrypt</code> <code>bool</code> <p>Whether to decrypt the weights.</p> <code>True</code> <code>provider</code> <code>Optional[str]</code> <p>Storage provider to load from. If None, try all available locations.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tuple[bytes, Dict[str, Any]]</code> <p>Tuple of (weights_data, metadata).</p> Source code in <code>src/models/secure_weights.py</code> <pre><code>def load_weights(\n    self, model_name: str, version: Optional[str] = None, decrypt: bool = True, provider: Optional[str] = None\n) -&gt; Tuple[bytes, Dict[str, Any]]:\n    \"\"\"Load model weights securely.\n\n    Args:\n        model_name: Name of the model.\n        version: Version of the weights. If None, the latest version is used.\n        decrypt: Whether to decrypt the weights.\n        provider: Storage provider to load from. If None, try all available locations.\n\n    Returns:\n        Tuple of (weights_data, metadata).\n    \"\"\"\n    # Normalize model name\n    model_name = model_name.replace(\" \", \"_\").lower()\n\n    # Get version if not specified\n    if version is None:\n        version = self._get_latest_version(model_name)\n        if version is None:\n            raise ValueError(f\"No versions found for model {model_name}\")\n\n    # Check if we have metadata for this model and version\n    weights_metadata = self._get_weights_metadata(model_name, version)\n    if not weights_metadata:\n        raise ValueError(f\"No metadata found for model {model_name} version {version}\")\n\n    # Check if weights are in cache\n    if self.cache_enabled and not provider:\n        cache_data = self._load_from_cache(model_name, version)\n        if cache_data is not None:\n            logger.info(f\"Loaded weights for {model_name} version {version} from cache\")\n            weights_data = cache_data\n\n            # Verify checksum\n            calculated_checksum = self._calculate_checksum(\n                weights_data\n                if not decrypt or not weights_metadata.get(\"encrypted\", False)\n                else self.encryption_service.decrypt_data(weights_data)\n            )\n\n            if calculated_checksum != weights_metadata[\"checksum\"]:\n                logger.warning(\n                    f\"Checksum mismatch for cached weights of {model_name} version {version}. \"\n                    f\"Expected {weights_metadata['checksum']}, got {calculated_checksum}. \"\n                    f\"Loading from storage instead.\"\n                )\n            else:\n                # Decrypt if required\n                if decrypt and weights_metadata.get(\"encrypted\", False):\n                    weights_data = self.encryption_service.decrypt_data(weights_data)\n\n                return weights_data, weights_metadata\n\n    # Load from specified provider or try all available locations\n    if provider:\n        providers_to_try = [provider]\n    else:\n        # Get providers from metadata\n        providers_to_try = [location[\"provider\"] for location in weights_metadata.get(\"storage_locations\", [])]\n\n    # Try to load from each provider\n    last_error = None\n    for provider_name in providers_to_try:\n        try:\n            weights_data = self._load_from_provider(provider_name, model_name, version, weights_metadata)\n\n            # Verify checksum\n            calculated_checksum = self._calculate_checksum(\n                weights_data\n                if not decrypt or not weights_metadata.get(\"encrypted\", False)\n                else self.encryption_service.decrypt_data(weights_data)\n            )\n\n            if calculated_checksum != weights_metadata[\"checksum\"]:\n                logger.warning(\n                    f\"Checksum mismatch for weights of {model_name} version {version} \"\n                    f\"from provider {provider_name}. Expected {weights_metadata['checksum']}, \"\n                    f\"got {calculated_checksum}. Trying another provider.\"\n                )\n                continue\n\n            # Store in cache if enabled\n            if self.cache_enabled:\n                self._store_in_cache(model_name, version, weights_data, weights_metadata)\n\n            # Decrypt if required\n            if decrypt and weights_metadata.get(\"encrypted\", False):\n                weights_data = self.encryption_service.decrypt_data(weights_data)\n\n            logger.info(f\"Loaded weights for {model_name} version {version} \" f\"from provider {provider_name}\")\n\n            return weights_data, weights_metadata\n\n        except Exception as e:\n            logger.error(\n                f\"Error loading weights for {model_name} version {version} \" f\"from provider {provider_name}: {e}\"\n            )\n            last_error = e\n\n    # If we get here, we failed to load from any provider\n    raise RuntimeError(\n        f\"Failed to load weights for {model_name} version {version} \" f\"from any provider: {last_error}\"\n    )\n</code></pre>"},{"location":"api/models/#src.models.secure_weights.SecureModelWeights.load_weights_to_file","title":"<code>load_weights_to_file(model_name, output_path, version=None, decrypt=True, provider=None)</code>","text":"<p>Load model weights to a file.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model.</p> required <code>output_path</code> <code>Union[str, Path]</code> <p>Path to save the weights.</p> required <code>version</code> <code>Optional[str]</code> <p>Version of the weights. If None, the latest version is used.</p> <code>None</code> <code>decrypt</code> <code>bool</code> <p>Whether to decrypt the weights.</p> <code>True</code> <code>provider</code> <code>Optional[str]</code> <p>Storage provider to load from. If None, try all available locations.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Metadata for the loaded weights.</p> Source code in <code>src/models/secure_weights.py</code> <pre><code>def load_weights_to_file(\n    self,\n    model_name: str,\n    output_path: Union[str, Path],\n    version: Optional[str] = None,\n    decrypt: bool = True,\n    provider: Optional[str] = None,\n) -&gt; Dict[str, Any]:\n    \"\"\"Load model weights to a file.\n\n    Args:\n        model_name: Name of the model.\n        output_path: Path to save the weights.\n        version: Version of the weights. If None, the latest version is used.\n        decrypt: Whether to decrypt the weights.\n        provider: Storage provider to load from. If None, try all available locations.\n\n    Returns:\n        Metadata for the loaded weights.\n    \"\"\"\n    # Load weights\n    weights_data, weights_metadata = self.load_weights(model_name, version, decrypt, provider)\n\n    # Save to file\n    output_path = Path(output_path)\n    with open(output_path, \"wb\") as f:\n        f.write(weights_data)\n\n    logger.info(\n        f\"Saved weights for {model_name} version {version} to {output_path}, \"\n        f\"size: {len(weights_data)/1024/1024:.2f} MB\"\n    )\n\n    return weights_metadata\n</code></pre>"},{"location":"api/models/#src.models.secure_weights.SecureModelWeights.store_weights","title":"<code>store_weights(model_name, weights_file, version=None, metadata=None, encrypt=True)</code>","text":"<p>Store model weights securely.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model.</p> required <code>weights_file</code> <code>Union[str, Path, BinaryIO]</code> <p>Path to weights file or file-like object.</p> required <code>version</code> <code>Optional[str]</code> <p>Version of the weights. If None, a timestamp-based version is used.</p> <code>None</code> <code>metadata</code> <code>Optional[Dict[str, Any]]</code> <p>Additional metadata for the weights.</p> <code>None</code> <code>encrypt</code> <code>bool</code> <p>Whether to encrypt the weights.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dictionary containing details of the stored weights.</p> Source code in <code>src/models/secure_weights.py</code> <pre><code>def store_weights(\n    self,\n    model_name: str,\n    weights_file: Union[str, Path, BinaryIO],\n    version: Optional[str] = None,\n    metadata: Optional[Dict[str, Any]] = None,\n    encrypt: bool = True,\n) -&gt; Dict[str, Any]:\n    \"\"\"Store model weights securely.\n\n    Args:\n        model_name: Name of the model.\n        weights_file: Path to weights file or file-like object.\n        version: Version of the weights. If None, a timestamp-based version is used.\n        metadata: Additional metadata for the weights.\n        encrypt: Whether to encrypt the weights.\n\n    Returns:\n        Dictionary containing details of the stored weights.\n    \"\"\"\n    # Generate version if not provided\n    if version is None:\n        version = f\"v_{int(time.time())}\"\n\n    # Normalize model name\n    model_name = model_name.replace(\" \", \"_\").lower()\n\n    # Get file data and calculate checksum\n    if isinstance(weights_file, (str, Path)):\n        weights_path = Path(weights_file)\n        if not weights_path.exists():\n            raise FileNotFoundError(f\"Weights file {weights_path} not found\")\n\n        with open(weights_path, \"rb\") as f:\n            weights_data = f.read()\n    else:\n        # File-like object\n        weights_data = weights_file.read()\n        if hasattr(weights_file, \"seek\"):\n            weights_file.seek(0)\n\n    # Calculate checksum\n    checksum = self._calculate_checksum(weights_data)\n\n    # Prepare metadata\n    weights_metadata = {\n        \"model_name\": model_name,\n        \"version\": version,\n        \"timestamp\": int(time.time()),\n        \"checksum\": checksum,\n        \"checksum_algorithm\": self.checksum_algorithm,\n        \"size_bytes\": len(weights_data),\n        \"encrypted\": encrypt,\n        \"storage_locations\": [],\n    }\n\n    # Add any additional metadata\n    if metadata:\n        weights_metadata.update(metadata)\n\n    # Encrypt weights if required\n    if encrypt:\n        weights_data = self.encryption_service.encrypt_data(weights_data)\n        weights_metadata[\"encrypted\"] = True\n\n    # Store in primary storage\n    primary_location = self._store_to_provider(\n        self.primary_storage, model_name, version, weights_data, weights_metadata\n    )\n    weights_metadata[\"storage_locations\"].append(primary_location)\n\n    # Replicate to other providers if configured\n    for provider in self.replicate_to:\n        try:\n            location = self._store_to_provider(provider, model_name, version, weights_data, weights_metadata)\n            weights_metadata[\"storage_locations\"].append(location)\n        except Exception as e:\n            logger.error(f\"Error replicating weights to {provider}: {e}\")\n\n    # Store in local cache if enabled\n    if self.cache_enabled:\n        self._store_in_cache(model_name, version, weights_data, weights_metadata)\n\n    # Update metadata registry\n    with self.metadata_lock:\n        if model_name not in self.metadata:\n            self.metadata[model_name] = {}\n        self.metadata[model_name][version] = weights_metadata\n\n    logger.info(\n        f\"Stored weights for {model_name} version {version}, \"\n        f\"size: {len(weights_data)/1024/1024:.2f} MB, \"\n        f\"checksum: {checksum[:8]}..., encrypted: {encrypt}\"\n    )\n\n    return weights_metadata\n</code></pre>"},{"location":"api/models/#src.models.secure_weights.SecureModelWeights.verify_weights_integrity","title":"<code>verify_weights_integrity(model_name, version=None, provider=None)</code>","text":"<p>Verify the integrity of model weights.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model.</p> required <code>version</code> <code>Optional[str]</code> <p>Version of the weights. If None, verify all versions.</p> <code>None</code> <code>provider</code> <code>Optional[str]</code> <p>Storage provider to verify. If None, verify all providers.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, bool]</code> <p>Dictionary mapping provider to verification result.</p> Source code in <code>src/models/secure_weights.py</code> <pre><code>def verify_weights_integrity(\n    self, model_name: str, version: Optional[str] = None, provider: Optional[str] = None\n) -&gt; Dict[str, bool]:\n    \"\"\"Verify the integrity of model weights.\n\n    Args:\n        model_name: Name of the model.\n        version: Version of the weights. If None, verify all versions.\n        provider: Storage provider to verify. If None, verify all providers.\n\n    Returns:\n        Dictionary mapping provider to verification result.\n    \"\"\"\n    # Normalize model name\n    model_name = model_name.replace(\" \", \"_\").lower()\n\n    # Get versions to verify\n    versions_to_verify = []\n    if version is None:\n        # Verify all versions\n        with self.metadata_lock:\n            if model_name in self.metadata:\n                versions_to_verify = list(self.metadata[model_name].keys())\n    else:\n        versions_to_verify = [version]\n\n    if not versions_to_verify:\n        logger.warning(f\"No versions found for model {model_name}\")\n        return {}\n\n    # Verify each version\n    results = {}\n    for ver in versions_to_verify:\n        # Get metadata\n        weights_metadata = self._get_weights_metadata(model_name, ver)\n        if not weights_metadata:\n            logger.warning(f\"No metadata found for model {model_name} version {ver}\")\n            continue\n\n        # Get providers to verify\n        if provider:\n            providers_to_verify = [provider]\n        else:\n            # Get providers from metadata\n            providers_to_verify = [\n                location[\"provider\"] for location in weights_metadata.get(\"storage_locations\", [])\n            ]\n\n        # Verify each provider\n        for provider_name in providers_to_verify:\n            try:\n                # Load weights\n                weights_data = self._load_from_provider(provider_name, model_name, ver, weights_metadata)\n\n                # Calculate checksum\n                if weights_metadata.get(\"encrypted\", False):\n                    # Decrypt first to get the original checksum\n                    decrypted_data = self.encryption_service.decrypt_data(weights_data)\n                    calculated_checksum = self._calculate_checksum(decrypted_data)\n                else:\n                    calculated_checksum = self._calculate_checksum(weights_data)\n\n                # Verify checksum\n                expected_checksum = weights_metadata.get(\"checksum\")\n                if calculated_checksum == expected_checksum:\n                    results[f\"{model_name}/{ver}/{provider_name}\"] = True\n                    logger.info(\n                        f\"Verified integrity of weights for {model_name} version {ver} \"\n                        f\"from provider {provider_name}: OK\"\n                    )\n                else:\n                    results[f\"{model_name}/{ver}/{provider_name}\"] = False\n                    logger.warning(\n                        f\"Integrity check failed for weights of {model_name} version {ver} \"\n                        f\"from provider {provider_name}. Expected {expected_checksum}, \"\n                        f\"got {calculated_checksum}.\"\n                    )\n            except Exception as e:\n                results[f\"{model_name}/{ver}/{provider_name}\"] = False\n                logger.error(\n                    f\"Error verifying weights for {model_name} version {ver} \" f\"from provider {provider_name}: {e}\"\n                )\n\n    return results\n</code></pre>"},{"location":"architecture/overview/","title":"Architecture Overview","text":"<p>HIPO is designed with a modular architecture that supports deployment across multiple cloud providers.</p>"},{"location":"architecture/overview/#core-components","title":"Core Components","text":""},{"location":"architecture/overview/#cloud-providers","title":"Cloud Providers","text":"<p>The platform abstracts away cloud-specific APIs through a unified provider interface, allowing seamless deployment to AWS, GCP, and other supported cloud environments.</p>"},{"location":"architecture/overview/#kubernetes-orchestration","title":"Kubernetes Orchestration","text":"<p>HIPO uses Kubernetes as the orchestration layer for deploying and managing ML workloads. This provides:</p> <ul> <li>Container-based deployment</li> <li>Horizontal and vertical scaling</li> <li>Resource management</li> <li>Service discovery</li> </ul>"},{"location":"architecture/overview/#model-management","title":"Model Management","text":"<p>The platform includes robust capabilities for:</p> <ul> <li>Secure storage of model weights</li> <li>Version control</li> <li>Access control</li> <li>Integrity verification</li> </ul>"},{"location":"architecture/overview/#autoscaling","title":"Autoscaling","text":"<p>The autoscaling system monitors resource utilization and automatically adjusts the cluster size based on workload demands.</p>"},{"location":"tutorials/getting-started/","title":"Getting Started","text":"<p>This guide will help you get started with HIPO for multi-cloud ML model deployment.</p>"},{"location":"tutorials/getting-started/#installation","title":"Installation","text":"<pre><code>pip install hipo\n</code></pre>"},{"location":"tutorials/getting-started/#configuration","title":"Configuration","text":"<p>Create a configuration file <code>config.yaml</code>:</p> <pre><code>cloud_providers:\n  aws:\n    enabled: true\n    region: us-west-2\n  gcp:\n    enabled: true\n    project_id: my-project\n    region: us-central1\n\nkubernetes:\n  cluster_name: ml-cluster\n  namespace: ml-models\n\nmodels:\n  storage:\n    primary: s3\n    s3_bucket: ml-model-weights\n    encryption_enabled: true\n</code></pre>"},{"location":"tutorials/getting-started/#basic-usage","title":"Basic Usage","text":"<pre><code>from hipo import MultiCloudOrchestrator\n\n# Initialize the orchestrator\norchestrator = MultiCloudOrchestrator(config=\"config.yaml\")\n\n# Deploy a model\norchestrator.deploy_model(\n    name=\"llama2-7b\",\n    version=\"v1.0\",\n    weights_path=\"/path/to/weights\",\n    replicas=3,\n    gpu_type=\"nvidia-a100\"\n)\n</code></pre>"},{"location":"tutorials/secure-model-deployment/","title":"Secure Model Deployment Tutorial","text":"<p>This tutorial demonstrates how to securely deploy an LLM model using HIPO's secure model weights storage and multi-cloud Kubernetes infrastructure.</p>"},{"location":"tutorials/secure-model-deployment/#prerequisites","title":"Prerequisites","text":"<ul> <li>HIPO installed and configured</li> <li>Access to at least one cloud provider (AWS, GCP, or Azure)</li> <li>A trained model to deploy</li> </ul>"},{"location":"tutorials/secure-model-deployment/#step-1-configure-secure-storage","title":"Step 1: Configure Secure Storage","text":"<p>First, ensure your configuration includes secure storage settings. Edit <code>config/default_config.yaml</code>:</p> <pre><code>security:\n  encryption:\n    key_directory: 'secrets'\n    load_keys_from_env: false\n\n  secure_weights:\n    enabled: true\n    storage:\n      primary: 's3'  # Choose your primary storage provider\n      replicate_to: ['gcs']  # Optional redundancy\n      s3_bucket: 'my-llm-models'\n      gcs_bucket: 'my-llm-models'\n      versioning_enabled: true\n      checksum_algorithm: 'sha256'\n      access_control_enabled: true\n      encryption_enabled: true\n\n    cache:\n      enabled: true\n      directory: 'weights_cache'\n      max_size_gb: 10\n\nsecrets:\n  model_weights:\n    storage_type: 's3'\n    s3_bucket: 'my-llm-models'\n    sync_enabled: true\n    versioning_enabled: true\n</code></pre>"},{"location":"tutorials/secure-model-deployment/#step-2-initialize-secure-storage-components","title":"Step 2: Initialize Secure Storage Components","text":"<p>Create a Python script <code>secure_deploy.py</code>:</p> <pre><code>import os\nfrom pathlib import Path\n\nfrom src.config.config import load_config\nfrom src.security.encryption import EncryptionService\nfrom src.models.secure_weights import create_secure_weights_manager\nfrom src.cloud.factory import CloudProviderFactory\nfrom src.secrets.secret_manager import SecretManager\nfrom src.kubernetes.orchestrator import KubernetesOrchestrator\n\n# Load configuration\nconfig = load_config(\"config/default_config.yaml\")\n\n# Initialize cloud providers\ncloud_factory = CloudProviderFactory(config)\ncloud_providers = {\n    \"aws\": cloud_factory.create_provider(\"aws\"),\n    \"gcp\": cloud_factory.create_provider(\"gcp\")\n}\n\n# Initialize encryption service\nencryption_service = EncryptionService(config.get(\"security\", {}).get(\"encryption\", {}))\n\n# Initialize secret manager\nsecret_manager = SecretManager(config, cloud_providers)\n\n# Initialize secure weights manager\nsecure_weights_config = secret_manager.manage_model_weights()\nsecure_weights = create_secure_weights_manager(\n    config={\"secure_weights\": secure_weights_config},\n    encryption_service=encryption_service,\n    secret_manager=secret_manager\n)\n\n# Initialize Kubernetes orchestrator\norchestrator = KubernetesOrchestrator(config, cloud_providers)\n</code></pre>"},{"location":"tutorials/secure-model-deployment/#step-3-upload-model-weights-securely","title":"Step 3: Upload Model Weights Securely","text":"<p>Add the following to your script:</p> <pre><code>def upload_model_weights(model_name, weights_path):\n    \"\"\"Upload model weights securely to multi-cloud storage.\"\"\"\n    # Generate a version based on timestamp\n    from datetime import datetime\n    version = f\"v_{int(datetime.now().timestamp())}\"\n\n    print(f\"Uploading {model_name} weights version {version}...\")\n\n    # Add metadata for the model\n    metadata = {\n        \"model_type\": \"llm\",\n        \"framework\": \"pytorch\",\n        \"parameters\": \"7B\",\n        \"trained_on\": datetime.now().isoformat(),\n        \"description\": \"Llama 2 fine-tuned model\"\n    }\n\n    # Store weights securely\n    result = secure_weights.store_weights(\n        model_name=model_name,\n        weights_file=weights_path,\n        version=version,\n        metadata=metadata,\n        encrypt=True  # Enable encryption\n    )\n\n    print(f\"Upload complete. Stored in {len(result['storage_locations'])} locations:\")\n    for location in result['storage_locations']:\n        print(f\"- {location['provider']}\")\n\n    return result\n\n# Path to your model weights\nmodel_weights_path = \"/path/to/llama2-7b-weights.bin\"\nmodel_name = \"llama2-7b-finetuned\"\n\n# Upload the model\nupload_result = upload_model_weights(model_name, model_weights_path)\n</code></pre>"},{"location":"tutorials/secure-model-deployment/#step-4-deploy-the-model-to-kubernetes","title":"Step 4: Deploy the Model to Kubernetes","text":"<p>Add the deployment code to your script:</p> <pre><code>def deploy_model_to_kubernetes(model_name, version, provider=\"aws\"):\n    \"\"\"Deploy the model to Kubernetes.\"\"\"\n    print(f\"Deploying {model_name} version {version} to Kubernetes...\")\n\n    # Create a deployment configuration\n    deployment_config = {\n        \"name\": model_name,\n        \"version\": version,\n        \"replicas\": 2,\n        \"resources\": {\n            \"requests\": {\n                \"cpu\": \"4\",\n                \"memory\": \"16Gi\",\n                \"nvidia.com/gpu\": \"1\"\n            },\n            \"limits\": {\n                \"cpu\": \"8\",\n                \"memory\": \"32Gi\",\n                \"nvidia.com/gpu\": \"1\"\n            }\n        },\n        \"env\": {\n            \"MAX_BATCH_SIZE\": \"8\",\n            \"MAX_SEQUENCE_LENGTH\": \"2048\",\n            \"TEMPERATURE\": \"0.7\"\n        },\n        \"secure_weights\": True,  # Enable secure weights loading\n        \"provider\": provider\n    }\n\n    # Deploy the model\n    deployment = orchestrator.deploy_model(\n        deployment_config=deployment_config,\n        weights_manager=secure_weights\n    )\n\n    print(f\"Deployment complete: {deployment['status']}\")\n    print(f\"API endpoint: {deployment['api_endpoint']}\")\n\n    return deployment\n\n# Deploy the model to Kubernetes\ndeployment = deploy_model_to_kubernetes(\n    model_name=model_name,\n    version=upload_result['version'],\n    provider=\"aws\"  # Primary provider\n)\n</code></pre>"},{"location":"tutorials/secure-model-deployment/#step-5-verify-the-deployment","title":"Step 5: Verify the Deployment","text":"<p>Add verification code:</p> <pre><code>def verify_deployment(deployment):\n    \"\"\"Verify the deployment is healthy.\"\"\"\n    import time\n    import requests\n\n    # Wait for deployment to be ready\n    print(\"Waiting for deployment to be ready...\")\n    for i in range(12):  # Wait up to 2 minutes\n        status = orchestrator.get_deployment_status(deployment['id'])\n        if status['status'] == 'Running':\n            print(\"Deployment is ready!\")\n            break\n        print(f\"Status: {status['status']} ({i+1}/12)\")\n        time.sleep(10)\n\n    # Check that the API endpoint is responding\n    try:\n        response = requests.get(\n            f\"{deployment['api_endpoint']}/health\",\n            timeout=5\n        )\n        if response.status_code == 200:\n            print(\"\u2705 API health check passed\")\n        else:\n            print(f\"\u274c API health check failed: {response.status_code}\")\n    except Exception as e:\n        print(f\"\u274c API health check failed: {e}\")\n\n    # Verify model weights integrity\n    integrity_results = secure_weights.verify_weights_integrity(\n        model_name=model_name,\n        version=upload_result['version']\n    )\n\n    all_verified = all(integrity_results.values())\n    if all_verified:\n        print(\"\u2705 Model weights integrity verified\")\n    else:\n        print(\"\u274c Model weights integrity check failed\")\n        for location, verified in integrity_results.items():\n            if not verified:\n                print(f\"  - Failed: {location}\")\n\n# Verify the deployment\nverify_deployment(deployment)\n</code></pre>"},{"location":"tutorials/secure-model-deployment/#step-6-run-the-script","title":"Step 6: Run the Script","text":"<p>Save and run the script:</p> <pre><code>python secure_deploy.py\n</code></pre>"},{"location":"tutorials/secure-model-deployment/#step-7-monitor-the-deployment","title":"Step 7: Monitor the Deployment","text":"<p>Use the Streamlit UI to monitor your deployment:</p> <pre><code>python src/ui/run_ui.py\n</code></pre> <p>Navigate to the Monitoring page to see: - Deployment status - Resource usage - API request metrics - Cost metrics</p>"},{"location":"tutorials/secure-model-deployment/#conclusion","title":"Conclusion","text":"<p>You've successfully: 1. Configured secure model weights storage 2. Uploaded a model with encryption and redundancy 3. Deployed the model to Kubernetes 4. Verified the deployment's health and integrity 5. Set up monitoring</p> <p>This approach ensures your model weights are securely stored, with encryption, integrity verification, and multi-cloud redundancy. The deployment is managed by Kubernetes for scalability and resilience.</p>"}]}